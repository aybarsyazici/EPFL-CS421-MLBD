{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIOEeGs_DFE0"
   },
   "source": [
    "# Solutions: MLBD final exam (Spring 2022)\n",
    "\n",
    "The exam questions are contained in this Jupyter Notebook. The `data` folder contains the data. \n",
    "\n",
    "The logistical details, rules, and guidelines pertaining to the exam are stated below.   \n",
    "\n",
    "### Timeline and Submission\n",
    "**Exam date:** July 6, 2022   \n",
    "**Exam start:** 15h15  \n",
    "**Exam end:** 18h15\n",
    "\n",
    "### Instructions\n",
    "This exam consists of two parts, a Moodle quiz with conceptual questions and programming exercises in this notebook. **Note that the Moodle quiz with the conceptual questions will be closed by 16h15, therefore please make sure to answer the conceptual questions within the first hour of the exam. To submit this notebook for the coding questions, you should upload it to Moodle (at the latest by 18h15).**\n",
    "\n",
    "In case of issues with Moodle, send your file named as \"SCIPER_Firstname_Lastname.ipynb\" via email to paola.mejia@epfl.ch, subject \"[MLBD] Exam notebook\".\n",
    "\n",
    "### Rules\n",
    "\n",
    "1. You are allowed to use any environment. We recommend using EPFL's Noto environment, accessible through the link: [https://noto.epfl.ch/](https://noto.epfl.ch/). We prepared a Python environment with all the Python packages that you might need for the exam, in the default EPFL's Noto installation. If you want to use some additional packages, feel free to install and use them in a virtual environment. In this case, it is your own responsibility to make sure that your environment is functional and that your results can be properly interpreted for grading. \n",
    "\n",
    "\n",
    "2. Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "3. When asked for plots, please include all the needed decorations: namely title, x/y-axis labels, appropriate x/y-ticks, legend, and so on. \n",
    "\n",
    "4. We will grade your notebook as is, which means that only the results showed in your evaluated code cells will be considered. Please be sure to submit a **fully-run and evaluated notebook**. We will not run the notebook for you. Interactive plots, such as those generated using `plotly`, should be **strictly avoided**.\n",
    "\n",
    "5. You can use all the online resources (including the code from the demo notebooks from the course) you want except for communication tools (emails, web chats, forums, phone, etc.). Remember, this is not a project assignment. Therefore, no teamwork is allowed.\n",
    "\n",
    "### Setup\n",
    "We intend this notebook to be completed on EPFL's Noto environment. As in past lecture exercises, you will need to use the `Tensorflow` kernel for the dependencies to be installed appropriately. Change the kernel in the upper right corner of Noto. Select `Tensorflow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jp4ln17R8BNF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_classif\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.manifold import spectral_embedding\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist, squareform, cdist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MnSVIV4Xu_wt",
    "tags": []
   },
   "source": [
    "## Question 1 (15 points)\n",
    "You are the Senior Data Scientist in a learning platform called LernTime. You have realized that many users stop using the platform and want to increase user retention. For this purpose, you decide to build a model to predict whether a student will stop using the learning platform or not.\n",
    "\n",
    "Your data science team built a data frame in which each row contains the aggregated features per student (calculated over the first 5 weeks of interactions) and the feature `dropout` indicates whether the student stopped using the platform (1) or not (0) before week 10.\n",
    "\n",
    "The dataframe is in the file `lerntime.csv` and contains the following features:\n",
    "- `video_time`: total video time (in minutes) \n",
    "- `num_sessions` total number of sessions\n",
    "- `num_quizzes`: total number of quizzes attempts\n",
    "- `reading_time`: total theory reading time\n",
    "- `previous_knowledge`: standardized previous knowledge\n",
    "- `browser_speed`: standardized browser speed\n",
    "- `device`:  whether the student logged in using a smartphone (1) or a computer (-1)\n",
    "- `topics`: the topics covered by the user\n",
    "- `education`: current level of education (0: middle school, 1: high school, 2: bachelor, 3: master, 4: Ph.D.).\n",
    "- `dropout`: whether the student stopped using the platform (1) or not (0) before week 5.\n",
    "\n",
    "The newest data scientist created two models with an excellent performance. As a Senior Data Scientist, you are suspicious of the results and decide to revise the code. \n",
    "\n",
    "Your task is to:\n",
    "\n",
    "a) Identify the mistakes. In the first cell, add a comment above each line in which you identify an error and explain the error.\n",
    "\n",
    "b) In the second cell, you must correct the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TtWJ4sDNvXQP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lerntime_dropout.csv')\n",
    "\n",
    "y = df['dropout']\n",
    "X = df[['video_time', 'num_sessions', 'num_quizzes', 'reading_time',\n",
    "       'previous_knowledge', 'browser_speed']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r85IEfMeCgq9"
   },
   "source": [
    "### a) Identify the mistakes in the code (10 points)\n",
    "In the following cell, add a comment above each line in which you identify an error and explain the why it is erroneous.\n",
    "Please start each of your comments with `#ERROR:`. For example:\n",
    "\n",
    "`#ERROR: the RMSE of the model is printed instead of the AUC`\n",
    "\n",
    "`print(\"The AUC of the model is: {}\".format(rmse))          `\n",
    "\n",
    "You may assume that: \n",
    "- all the features are continous and numerical. \n",
    "- the features have already been cleaned and processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Or-X9lyFFVAm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6)\n",
      "(300, 3)\n",
      "Score model 1: 0.31\n",
      "Score model 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ERROR: Train-test split should be done before preprocessing steps 1. and 2. to avoid data leakage, \n",
    "# fitting both scaler and selector only on X_train\n",
    "## 1. Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "## 2. Feature selection (Lasso)\n",
    "print(X.shape)\n",
    "lasso = Lasso(alpha=0.1, random_state=0).fit(X, y)\n",
    "selector = SelectFromModel(lasso, prefit = True)\n",
    "X = selector.transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "## 3. Split the data\n",
    "# ERROR: Splitting should be stratified on y, since the dataset is unbalanced\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "## Model 1\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=0)\n",
    "# ERROR: Fit should only be called on the train set\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X_test)\n",
    "# ERROR: The adjusted mutual information is not an appropriate score for classification, since it would give\n",
    "# a perfect score even if the predictions are the complete opposite of y_test\n",
    "print(\"Score model 1: {}\".format(np.round(adjusted_mutual_info_score(preds, y_test), 2)))\n",
    "\n",
    "## Model 2\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=0)\n",
    "# ERROR: Fit should only be called on the train set\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X_test)\n",
    "# ERROR: The adjusted mutual information is not an appropriate score for classification, since it would give\n",
    "# a perfect score even if the predictions are the complete opposite of y_test\n",
    "print(\"Score model 2: {}\".format(np.round(adjusted_mutual_info_score(preds, y_test), 2)))\n",
    "\n",
    "# ERROR: The second model has just more complexity and can hence better overfit to the test set, which leaked during training\n",
    "## Discussion\n",
    "# Our second model achieved perfect results with unseen data and outperforms the first model.\n",
    "## This is because we increased the number of estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "acDnbzGiCnBD"
   },
   "source": [
    "### b) Correct the code (5 points)\n",
    "Correct all the erroneous code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7wkyo3yACcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 3)\n",
      "(240, 3)\n",
      "Score model 1: 0.74\n",
      "Score model 2: 0.81\n"
     ]
    }
   ],
   "source": [
    "## 1. Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=0)\n",
    "\n",
    "## 2. Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## 3. Feature selection (Lasso)\n",
    "print(X_train.shape)\n",
    "lasso = Lasso(alpha=0.1, random_state=0).fit(X_train, y_train)\n",
    "selector = SelectFromModel(lasso, prefit = True)\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)\n",
    "print(X_train.shape)\n",
    "\n",
    "## Model 1\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Score model 1: {}\".format(np.round(balanced_accuracy_score(preds, y_test), 2)))\n",
    "\n",
    "## Model 2\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Score model 2: {}\".format(np.round(balanced_accuracy_score(preds, y_test), 2)))\n",
    "\n",
    "## Discussion\n",
    "# Our first model achieved good results with unseen data, outperforming the second model.\n",
    "# This might be because, without limiting the depth of each tree, the second model is overfitting more to the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5Eco8WSdxOJR"
   },
   "source": [
    "# Question 2 (5 points)\n",
    "You decide to explore the data further. You are especially interested in the two features `device` and `education` and decide to explore the relationship between them.  \n",
    "\n",
    "What is the relationship between the two features `device` and `education`? Support your answer with informative metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    268\n",
       "0.0     32\n",
       "Name: device, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.device.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oCP4jWrE-hXV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education  device\n",
       "0.0        0.0       1.0\n",
       "1.0        1.0       1.0\n",
       "2.0        1.0       1.0\n",
       "3.0        1.0       1.0\n",
       "4.0        1.0       1.0\n",
       "Name: device, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('education').device.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='education', hue='device', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.education.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.device.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33948894])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_classif(df.education.values.reshape(-1, 1), df.device.values, discrete_features=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5KVg8APc-hXW"
   },
   "source": [
    "It seems that users with higher education tend to use computers instead of smartphones, possibly indicating a more serious/dedicated approach to learning with respect to middle and high school students for which browsing the platform is more of a casual activity. This interpretation is coherent with the mutual information computed between the two features (~0.34), which indicates a relevant correlation between them. In fact, if we have information about the device, we can make a better prediction for education, and viceversa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dkj2DKz4ymMt"
   },
   "source": [
    "# Question 3 (40 points)\n",
    "\n",
    "After having looked in more detail into the features, you decide to explore the different type of users. You want to use your knowledge from your ML4BD course and decide to cluster using Spectral Clustering. In the course, you learnt different ways of constructing the similarity graph, yielding the adjacency matrix serving as an input to the Spectral Clustering. Based on your in-depth exploration of the data, you decide to construct the similarity graph as a  *k-nearest neighbor graph*.\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "a) Write a function to compute the k-nearest neighbor graph.\n",
    "\n",
    "b) Cluster the users using Spectral Clustering and your k-nearest neighbor graph function (use 4 neighbors). Use only the features *reading_time* and *topics*. You can assume that optimal number of clusters is 2.\n",
    "\n",
    "c) Discuss the fairness of the obtained cluster solution regarding the level of education ('education')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NeRJlAdQR8Bz"
   },
   "source": [
    "## a) Computation of the k-nearest neighbor graph (17 points)\n",
    "Unfortunately, there is no k-nearest neighbor graph implementation available in scikit-learn and you therefore have to implement the function yourself. The function `'k_nearest_neighbor_graph'` takes a similarity matrix `S` as well as the number of neighbors `k` as an input an returns the adjacency matrix `W`.\n",
    "\n",
    "Note that we will not evaluate the coding efficiency of your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def my_k_nearest_neighbor_graph(S, k):\n",
    "    # Connect node x to node y if x is among the k nearest neighbors of y or vice versa\n",
    "    # Connection strength is the similarity between x and y, i.e. S[x,y]\n",
    "    # S: similarity matrix\n",
    "    # k: number of nearest neighbors\n",
    "    # return: adjacency matrix\n",
    "    S = np.array(S)\n",
    "    A = np.zeros(S.shape)\n",
    "    for i in range(S.shape[0]):\n",
    "        top_k = np.argsort(S[i, :])[-(k+1):]\n",
    "        A[i, top_k] = S[i, top_k]\n",
    "        A[top_k, i] = S[top_k, i]\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GZKWiCDZz5uS"
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbor_graph(S, k):\n",
    "    # S: similarity matrix\n",
    "    # k: number of neighbors\n",
    "   \n",
    "    S = np.array(S)\n",
    "    # k+1 because include_self. -S to pass from similarity to distance, +translation to avoid negative values\n",
    "    G = kneighbors_graph(-S + S.max(), k+1, metric='precomputed', mode='connectivity', include_self=True).toarray()\n",
    "    W = (G + G.T).astype(bool) * S\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.  1.  0. ]\n",
      " [0.2 1.  0.8 0.4]\n",
      " [0.7 0.8 1.  0.6]\n",
      " [0.  0.4 0.6 1. ]]\n",
      "[[0.  1.  1.  1. ]\n",
      " [0.2 1.  0.8 0.4]\n",
      " [0.7 0.8 1.  0.6]\n",
      " [0.1 0.4 0.6 1. ]]\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "# Please run this cell for evaluation purposes\n",
    "S = [[1, 1, 1, 1],\n",
    "     [0.2, 1, 0.8, 0.4],\n",
    "     [0.7, 0.8, 1, 0.6],\n",
    "     [0.1, 0.4, 0.6, 1]]\n",
    "\n",
    "print(k_nearest_neighbor_graph(S, k))\n",
    "print(my_k_nearest_neighbor_graph(S, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "rlRJFFE80Gag",
    "outputId": "d8540c1f-bb29-4f64-a79e-755450004620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.2 0.7 0. ]\n",
      " [0.2 1.  0.8 0.4]\n",
      " [0.7 0.8 1.  0.6]\n",
      " [0.  0.4 0.6 1. ]]\n",
      "[[1.  0.2 0.7 0. ]\n",
      " [0.2 1.  0.8 0.4]\n",
      " [0.7 0.8 1.  0.6]\n",
      " [0.  0.4 0.6 1. ]]\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "# Please run this cell for evaluation purposes\n",
    "S = [[1, 0.2, 0.7, 0.1],\n",
    "     [0.2, 1, 0.8, 0.4],\n",
    "     [0.7, 0.8, 1, 0.6],\n",
    "     [0.1, 0.4, 0.6, 1]]\n",
    "\n",
    "print(k_nearest_neighbor_graph(S, k))\n",
    "print(my_k_nearest_neighbor_graph(S, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "YEkEnF930MH7",
    "outputId": "9869bb87-a80d-4904-e13c-dbd60da234e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.3 0.  0.1]\n",
      " [0.3 1.  0.8 0.9]\n",
      " [0.  0.8 1.  0.6]\n",
      " [0.1 0.9 0.6 1. ]]\n",
      "[[1.  0.3 0.  0.1]\n",
      " [0.3 1.  0.8 0.9]\n",
      " [0.  0.8 1.  0.6]\n",
      " [0.1 0.9 0.6 1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Please run this cell for evaluation purposes\n",
    "S = [[1, 0.3, 0.01, 0.1],\n",
    "     [0.3, 1, 0.8, 0.9],\n",
    "     [0.01, 0.8, 1, 0.6],\n",
    "     [0.1, 0.9, 0.6, 1]]\n",
    "\n",
    "print(k_nearest_neighbor_graph(S, k))\n",
    "print(my_k_nearest_neighbor_graph(S, k))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DDNtQopEmTdA"
   },
   "source": [
    "## b) Spectral Clustering (15 points)\n",
    "Perform a spectral clustering using a k-nearest neighbor graph (with 4 neighbors). Use the two features `reading_time` and `topics` only. If you did not manage to solve task 3a), use a *fully connected graph* as similarity graph to obtain the adjacency matrix `W`. You can assume that the optimal number of clusters is 2. Print the obtained cluster labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Lf-d-ZLyAuA_"
   },
   "outputs": [],
   "source": [
    "# Function for doing spectral clustering\n",
    "def spectral_clustering(W, n_clusters, random_state=111):\n",
    "    \"\"\"\n",
    "    Spectral clustering\n",
    "    :param W: np array of adjacency matrix\n",
    "    :param n_clusters: number of clusters\n",
    "    :param normed: normalized or unnormalized Laplacian\n",
    "    :return: tuple (kmeans, proj_X, eigenvals_sorted)\n",
    "        WHERE\n",
    "        kmeans scikit learn clustering object\n",
    "        proj_X is np array of transformed data points\n",
    "        eigenvals_sorted is np array with ordered eigenvalues \n",
    "        \n",
    "    \"\"\"\n",
    "    # Compute eigengap heuristic\n",
    "    L = laplacian(W, normed=True)\n",
    "    eigenvals, _ = linalg.eig(L)\n",
    "    eigenvals = np.real(eigenvals)\n",
    "    eigenvals_sorted = eigenvals[np.argsort(eigenvals)]\n",
    "\n",
    "    # Create embedding\n",
    "    random_state = np.random.RandomState(random_state)\n",
    "    proj_X = spectral_embedding(W, n_components=n_clusters,\n",
    "                              random_state=random_state,\n",
    "                              drop_first=False)\n",
    "\n",
    "    # Cluster the points using k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state = random_state)\n",
    "    kmeans.fit(proj_X)\n",
    "\n",
    "    return kmeans, proj_X, eigenvals_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.875     , 0.875     , ..., 0.75      , 0.625     ,\n",
       "        0.875     ],\n",
       "       [0.875     , 1.        , 1.        , ..., 0.85714286, 0.5       ,\n",
       "        0.75      ],\n",
       "       [0.875     , 1.        , 1.        , ..., 0.85714286, 0.5       ,\n",
       "        0.75      ],\n",
       "       ...,\n",
       "       [0.75      , 0.85714286, 0.85714286, ..., 1.        , 0.375     ,\n",
       "        0.625     ],\n",
       "       [0.625     , 0.5       , 0.5       , ..., 0.375     , 1.        ,\n",
       "        0.71428571],\n",
       "       [0.875     , 0.75      , 0.75      , ..., 0.625     , 0.71428571,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_aybars = cdist(topics,topics, metric=lambda x, y: float(len(x[0].intersection(y[0])) / len(x[0].union(y[0]))))\n",
    "print(temp_aybars.shape)\n",
    "temp_aybars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =df[['reading_time']]\n",
    "S1 = pairwise_kernels(time, metric='rbf', gamma=1)\n",
    "\n",
    "topics = df[['topics']].apply(lambda x: set(eval(x.topics)), axis=1).to_numpy().reshape(-1, 1)\n",
    "S2 = squareform(pdist(topics, metric=lambda x, y: float(len(x[0].intersection(y[0])) / len(x[0].union(y[0])))))\n",
    "\n",
    "# Set diagonal to 1\n",
    "gen = tuple([i for i in range(S2.shape[0])])\n",
    "S2[gen, gen] = 1\n",
    "\n",
    "\n",
    "S = (S1 + S2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(S2, temp_aybars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Compute W\n",
    "k = 4\n",
    "W = k_nearest_neighbor_graph(S, 4)\n",
    "\n",
    "# Call the spectral clustering function and print out the labels\n",
    "clusters =2\n",
    "kmeans, proj_X, eigenvals_sorted = spectral_clustering(W, clusters)\n",
    "y_pred = kmeans.labels_\n",
    "print(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "28wBLO3ugafD"
   },
   "source": [
    "## c) Fairness of clustering solution (8 points)\n",
    "Some students approach you and say that your clustering algorithm is not fair with respect to the education level (specified in feature `education level`). You therefore decide to investigate the fairness of the obtained clustering solution. To do so, you choose an appropriate fairness metric, implement it, and apply it to compute the fairness of your clustering solution. Your further decide to visualize the obtained results in an informative manner, as a basis for your discussion with the students."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JWlX7rIp-hXY"
   },
   "source": [
    "Choose an appropriate fairness metric, implement it, and apply it to compute the fairness of your clustering solution. Justify your choice of metric and discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "9oDSPcKRtpLJ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education\n",
       "0.0    0.062500\n",
       "1.0    0.115385\n",
       "2.0    0.064935\n",
       "3.0    0.082353\n",
       "4.0    0.129630\n",
       "Name: y_pred, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fairness = df.copy()\n",
    "df_fairness.insert(0, 'y_pred', y_pred)\n",
    "\n",
    "prob_per_education = df_fairness.groupby('education').y_pred.mean()\n",
    "prob_per_education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>video_time</th>\n",
       "      <th>num_sessions</th>\n",
       "      <th>num_quizzes</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>previous_knowledge</th>\n",
       "      <th>browser_speed</th>\n",
       "      <th>device</th>\n",
       "      <th>topics</th>\n",
       "      <th>education</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45.793303</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>48.186562</td>\n",
       "      <td>1.675972</td>\n",
       "      <td>-0.294704</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Locke', 'Descartes', 'Socrates', 'Kant', 'Ni...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>51.331242</td>\n",
       "      <td>57.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.945810</td>\n",
       "      <td>0.700522</td>\n",
       "      <td>1.253694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Nietzche', 'Locke', 'Confucius', 'Aristotle'...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>87.414834</td>\n",
       "      <td>52.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.611978</td>\n",
       "      <td>1.836716</td>\n",
       "      <td>-1.171352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Plato', 'Locke', 'Nietzche', 'Socrates', 'De...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>58.556388</td>\n",
       "      <td>47.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.785805</td>\n",
       "      <td>0.209577</td>\n",
       "      <td>-2.043047</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Aristotle', 'Socrates', 'Plato', 'Confucius'...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>74.822362</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>38.907983</td>\n",
       "      <td>0.265678</td>\n",
       "      <td>-0.754559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Kant', 'Aristotle', 'Confucius', 'Locke', 'P...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0</td>\n",
       "      <td>87.777943</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.590844</td>\n",
       "      <td>-0.411104</td>\n",
       "      <td>0.763358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Aristotle', 'Confucius', 'Plato', 'Kant', 'N...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0</td>\n",
       "      <td>94.181158</td>\n",
       "      <td>54.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>42.503553</td>\n",
       "      <td>0.224276</td>\n",
       "      <td>-0.349900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Confucius', 'Descartes', 'Kant', 'Locke', 'S...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0</td>\n",
       "      <td>56.244754</td>\n",
       "      <td>51.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>56.969505</td>\n",
       "      <td>-0.688192</td>\n",
       "      <td>-0.651012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Socrates', 'Plato', 'Descartes', 'Aristotle'...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0</td>\n",
       "      <td>10.630718</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>67.457289</td>\n",
       "      <td>2.260416</td>\n",
       "      <td>2.467754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Plato', 'Socrates', 'Locke', 'Aristotle', 'K...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0</td>\n",
       "      <td>65.246548</td>\n",
       "      <td>77.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.124174</td>\n",
       "      <td>-0.123980</td>\n",
       "      <td>-0.019765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Locke', 'Confucius', 'Plato', 'Aristotle', '...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_pred  video_time  num_sessions  num_quizzes  reading_time  \\\n",
       "0         0   45.793303          99.0         36.0     48.186562   \n",
       "1         0   51.331242          57.0         12.0     49.945810   \n",
       "2         0   87.414834          52.0          7.0     20.611978   \n",
       "3         0   58.556388          47.0         31.0     33.785805   \n",
       "4         0   74.822362          58.0         37.0     38.907983   \n",
       "..      ...         ...           ...          ...           ...   \n",
       "295       0   87.777943          65.0          3.0     32.590844   \n",
       "296       0   94.181158          54.0         27.0     42.503553   \n",
       "297       0   56.244754          51.0         29.0     56.969505   \n",
       "298       0   10.630718          42.0         42.0     67.457289   \n",
       "299       0   65.246548          77.0         64.0     20.124174   \n",
       "\n",
       "     previous_knowledge  browser_speed  device  \\\n",
       "0              1.675972      -0.294704     1.0   \n",
       "1              0.700522       1.253694     1.0   \n",
       "2              1.836716      -1.171352     1.0   \n",
       "3              0.209577      -2.043047     1.0   \n",
       "4              0.265678      -0.754559     1.0   \n",
       "..                  ...            ...     ...   \n",
       "295           -0.411104       0.763358     1.0   \n",
       "296            0.224276      -0.349900     0.0   \n",
       "297           -0.688192      -0.651012     1.0   \n",
       "298            2.260416       2.467754     0.0   \n",
       "299           -0.123980      -0.019765     1.0   \n",
       "\n",
       "                                                topics  education  dropout  \n",
       "0    ['Locke', 'Descartes', 'Socrates', 'Kant', 'Ni...        2.0        0  \n",
       "1    ['Nietzche', 'Locke', 'Confucius', 'Aristotle'...        3.0        0  \n",
       "2    ['Plato', 'Locke', 'Nietzche', 'Socrates', 'De...        4.0        0  \n",
       "3    ['Aristotle', 'Socrates', 'Plato', 'Confucius'...        3.0        0  \n",
       "4    ['Kant', 'Aristotle', 'Confucius', 'Locke', 'P...        4.0        0  \n",
       "..                                                 ...        ...      ...  \n",
       "295  ['Aristotle', 'Confucius', 'Plato', 'Kant', 'N...        4.0        0  \n",
       "296  ['Confucius', 'Descartes', 'Kant', 'Locke', 'S...        0.0        0  \n",
       "297  ['Socrates', 'Plato', 'Descartes', 'Aristotle'...        3.0        0  \n",
       "298  ['Plato', 'Socrates', 'Locke', 'Aristotle', 'K...        0.0        1  \n",
       "299  ['Locke', 'Confucius', 'Plato', 'Aristotle', '...        1.0        0  \n",
       "\n",
       "[300 rows x 11 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fairness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TKc2lczt-hXY"
   },
   "source": [
    "Justify your choice of metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIfkqCpa-hXZ"
   },
   "source": [
    "The only metric, amongst the ones considered during the course, that can be appropriate for clustering, where we don't have true labels, is Demographic Parity. In fact, we want to investigate whether the cluster assignment is biased by education, i.e. if the probability of being assigned to cluster 1 is different amongst different education levels. In this case, since education can assume 5 values, the metric compares probabilities between all possible values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XtKbxUQ3-hXZ"
   },
   "source": [
    "Visualize your results in an informative manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jdF6fv8fuPny",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='education', y='y_pred', data=prob_per_education.reset_index())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3q0UhnhUuV-i"
   },
   "source": [
    "Is your clustering solution fair? If yes, why? If not, why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WgY-IlIa-hXZ"
   },
   "source": [
    "It seems that the students are right in saying that our clustering is somehow biased by the education level. In fact, universitary users are more likely assigned to cluster 1, while all middle-school students belong to cluster 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "THlqbmP_-hXZ",
    "tags": []
   },
   "source": [
    "# Question 4 (30 points)\n",
    "To improve course quality, the CEO of LernTime decides to adapt the difficulty level of the tasks to the knowledge of the students. She asks you to develop a type of knowledge tracing model able to predict the number of points a student will get on the next problem, based on the observed performance (in terms of points) on all the past problems.\n",
    "You are provided with an example data set from a mathematics course, containing the following columns: \n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| user_id | The ID of the student who is solving the problem.  | |\n",
    "| order_id | The temporal ID (timestamp) associated with the student's answer to the problem.  | |\n",
    "| relative_week | The week # since the student's first interaction with the platform.   | |\n",
    "| problem_id | The ID associated with the problem. | |\n",
    "| score | The student's performance on the problem in terms of obtained points. The maximum number of points is 10 and the minimum number of points is 0.\n",
    "\n",
    "You decide to use a Deep Knowledge Tracing model (with an LSTM layer). Unfortunately, you cannot directly use a standard DKT architecture as:\n",
    "\n",
    "1. Your data does not have skill names or IDs available, so you will have to modify DKT to predict based on problem IDs.\n",
    "\n",
    "2. Instead of predicting a binary outcome (right/wrong), your goal is to predict the number of obtained points (score).\n",
    "\n",
    "Your tasks therefore are to:\n",
    "\n",
    "a) Implement and evaluate an adjusted version of a DKT model able to predict the number of points a student will obtain on a problem.\n",
    "\n",
    "b) Justify and discuss all your design choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "5fHPSwAx-hXZ",
    "outputId": "a96a12d5-08d3-40cb-9c0d-4cb62e41efc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>relative_week</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94400</td>\n",
       "      <td>163987467</td>\n",
       "      <td>0</td>\n",
       "      <td>6473</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94400</td>\n",
       "      <td>164499411</td>\n",
       "      <td>5</td>\n",
       "      <td>11893</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94987</td>\n",
       "      <td>172053491</td>\n",
       "      <td>0</td>\n",
       "      <td>37570</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94987</td>\n",
       "      <td>172092447</td>\n",
       "      <td>0</td>\n",
       "      <td>7195</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94987</td>\n",
       "      <td>172243850</td>\n",
       "      <td>2</td>\n",
       "      <td>5945</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31653</th>\n",
       "      <td>361511</td>\n",
       "      <td>175196995</td>\n",
       "      <td>1</td>\n",
       "      <td>164496</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31654</th>\n",
       "      <td>361770</td>\n",
       "      <td>175148600</td>\n",
       "      <td>0</td>\n",
       "      <td>37570</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31655</th>\n",
       "      <td>361770</td>\n",
       "      <td>175280418</td>\n",
       "      <td>1</td>\n",
       "      <td>39162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31656</th>\n",
       "      <td>361771</td>\n",
       "      <td>175138828</td>\n",
       "      <td>0</td>\n",
       "      <td>39162</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31657</th>\n",
       "      <td>361771</td>\n",
       "      <td>175280271</td>\n",
       "      <td>1</td>\n",
       "      <td>6921</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31658 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   order_id  relative_week  problem_id  score\n",
       "0        94400  163987467              0        6473   10.0\n",
       "1        94400  164499411              5       11893   10.0\n",
       "2        94987  172053491              0       37570   10.0\n",
       "3        94987  172092447              0        7195   10.0\n",
       "4        94987  172243850              2        5945   10.0\n",
       "...        ...        ...            ...         ...    ...\n",
       "31653   361511  175196995              1      164496   10.0\n",
       "31654   361770  175148600              0       37570    6.5\n",
       "31655   361770  175280418              1       39162    0.0\n",
       "31656   361771  175138828              0       39162   10.0\n",
       "31657   361771  175280271              1        6921    0.0\n",
       "\n",
       "[31658 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df = pd.read_csv('data/lerntime_kt.csv')\n",
    "student_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo1L9bbD-hXZ"
   },
   "source": [
    "### a) Implementation of (adjusted) DKT model (20 points)\n",
    "\n",
    "Fortunately, you already have code available (below) for properly training and evaluating a standard DKT model. **Modify** this code to be able to predict the number of points a student will get on the next problem, based on the observed performance (in terms of points) on all the past problems. Note that the code in its current format **will not run properly** due to the two differences mentioned above: the data set at hand does not have skill names available and your model needs to predict the total number of obtained points for a problem instead of a binary outcome.\n",
    "\n",
    "Train your model for 10 epochs, and use the best model callback to find the optimal model. Evaluate your model using **appropriate performance metric(s)** - please print your metric(s). You do not need to tune the hyperparameters of your model for this task; instead use the following settings (already provided in the code below):\n",
    "\n",
    "```\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OuWAHJDj-hXZ"
   },
   "outputs": [],
   "source": [
    "# MODIFY THE CODE BELOW TO ADJUST THE DKT IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting the data into a training and test set\n",
    "def create_iterator(data):\n",
    "    '''\n",
    "    Create an iterator to split interactions in data into train and test, with the same student not appearing in two diverse folds.\n",
    "    :param data:        Dataframe with student's interactions.\n",
    "    :return:            An iterator.\n",
    "    '''    \n",
    "    # Both passing a matrix with the raw data or just an array of indexes works\n",
    "    X = np.arange(len(data.index))\n",
    "    # Groups of interactions are identified by the user id (we do not want the same user appearing in two folds)\n",
    "    groups = data['user_id'].values \n",
    "    return model_selection.GroupShuffleSplit(n_splits=1, train_size=.8, test_size=0.2, random_state=0).split(X, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "NPOGv7vU-hXa"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters are fixed!\n",
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "0fdSScjJ-hXZ"
   },
   "outputs": [],
   "source": [
    "# Functions for building the Tensorflow input sequences for the model\n",
    "def prepare_seq(df):\n",
    "    '''\n",
    "    Extract user_id sequence in preparation for DKT. The output of this function \n",
    "    feeds into the prepare_data() function. \n",
    "    '''\n",
    "    # Enumerate problem id as a categorical variable \n",
    "    # (i.e. [32, 12, 32, 45] -> [0, 1, 0, 2])\n",
    "    df['skill'], skill_codes = pd.factorize(df['problem_id'], sort=True)\n",
    "\n",
    "    # Cross skill id with answer to form a synthetic feature\n",
    "    df['skill_with_answer'] = df['skill'] * 2 + df['score']\n",
    "\n",
    "    # Convert to a sequence per user_id and shift features 1 timestep\n",
    "    seq = df.groupby('user_id').apply(lambda r: (r['skill_with_answer'].values[:-1], r['skill'].values[1:], r['score'].values[1:],))\n",
    "    \n",
    "    # Get max skill depth and max feature depth\n",
    "    skill_depth = df['skill'].max() \n",
    "    features_depth = df['skill_with_answer'].max() + 1\n",
    "\n",
    "    return seq, int(features_depth), int(skill_depth)\n",
    "\n",
    "def prepare_data(seq, params, features_depth, skill_depth):\n",
    "    '''\n",
    "    Manipulate the data sequences into the right format for DKT with padding by batch\n",
    "    and encoding categorical features.\n",
    "    '''\n",
    "    \n",
    "    # Get Tensorflow Dataset\n",
    "    dataset = tf.data.Dataset.from_generator(generator=lambda: seq, output_types=(tf.int32, tf.int32, tf.float32))\n",
    "\n",
    "    # Encode categorical features and merge skills with labels to compute target loss\n",
    "    dataset = dataset.map(\n",
    "        lambda feat, skill, label: (\n",
    "            tf.one_hot(feat, depth=features_depth),\n",
    "            tf.concat(values=[tf.one_hot(skill, depth=skill_depth), tf.expand_dims(label, -1)], axis=-1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Pad sequences to the appropriate length per batch\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=params['batch_size'],\n",
    "        padding_values=(params['mask_value'], params['mask_value']),\n",
    "        padded_shapes=([None, None], [None, None]),\n",
    "        drop_remainder=True\n",
    "    )\n",
    "\n",
    "    return dataset.repeat(), len(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the Tensorflow output sequences for the model\n",
    "def get_target(y_true, y_pred, mask_value=params['mask_value']):\n",
    "    ''' \n",
    "    Adjust y_true and y_pred to ignore predictions made using padded values.\n",
    "    '''\n",
    "    # Get skills and labels from y_true\n",
    "    mask = 1. - tf.cast(tf.equal(y_true, mask_value), y_true.dtype)\n",
    "    y_true = y_true * mask\n",
    "\n",
    "    problems, y_true = tf.split(y_true, num_or_size_splits=[-1, 1], axis=-1)\n",
    "\n",
    "    # Get predictions for each skill\n",
    "    y_pred = tf.reduce_sum(y_pred * problems, axis=-1, keepdims=True)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "-sUNNRrp-hXa"
   },
   "outputs": [],
   "source": [
    "# Obtain indexes for training and test sets\n",
    "train_index, test_index = next(create_iterator(student_df))\n",
    "\n",
    "# Split the data into training and test\n",
    "X_train, X_test = student_df.iloc[train_index], student_df.iloc[test_index]\n",
    "\n",
    "# Obtain indexes for training and validation sets\n",
    "train_val_index, val_index = next(create_iterator(X_train))\n",
    "\n",
    "# Split the training data into training and validation\n",
    "X_train_val, X_val = X_train.iloc[train_val_index], X_train.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>relative_week</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>score</th>\n",
       "      <th>skill</th>\n",
       "      <th>skill_with_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94400</td>\n",
       "      <td>163987467</td>\n",
       "      <td>0</td>\n",
       "      <td>6473</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94400</td>\n",
       "      <td>164499411</td>\n",
       "      <td>5</td>\n",
       "      <td>11893</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   order_id  relative_week  problem_id  score  skill  \\\n",
       "0    94400  163987467              0        6473   10.0     15   \n",
       "1    94400  164499411              5       11893   10.0     70   \n",
       "\n",
       "   skill_with_answer  \n",
       "0               40.0  \n",
       "1              150.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df[student_df.user_id == 94400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>relative_week</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>score</th>\n",
       "      <th>skill</th>\n",
       "      <th>skill_with_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21407</th>\n",
       "      <td>329470</td>\n",
       "      <td>170566632</td>\n",
       "      <td>0</td>\n",
       "      <td>6913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   order_id  relative_week  problem_id  score  skill  \\\n",
       "21407   329470  170566632              0        6913    0.0     20   \n",
       "\n",
       "       skill_with_answer  \n",
       "21407               40.0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df[(student_df.skill_with_answer == 40) & (student_df.skill != 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10. ,  0. ,  3.5,  6.5,  8. ,  2. ])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df.score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill_with_answer</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>{0, 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>{1, 6}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>{2, 7}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>{8, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>{9, 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>190.0</td>\n",
       "      <td>{90, 95}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>192.0</td>\n",
       "      <td>{96, 91}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>194.0</td>\n",
       "      <td>{97, 92}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>196.0</td>\n",
       "      <td>{98, 93}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>198.0</td>\n",
       "      <td>{99, 94}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    skill_with_answer         0\n",
       "0                10.0    {0, 5}\n",
       "1                12.0    {1, 6}\n",
       "2                14.0    {2, 7}\n",
       "3                16.0    {8, 3}\n",
       "4                18.0    {9, 4}\n",
       "..                ...       ...\n",
       "84              190.0  {90, 95}\n",
       "85              192.0  {96, 91}\n",
       "86              194.0  {97, 92}\n",
       "87              196.0  {98, 93}\n",
       "88              198.0  {99, 94}\n",
       "\n",
       "[89 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aybars_df = student_df.groupby('skill_with_answer').apply(lambda x: set(x.skill) if len(set(x.skill)) > 1 else None).dropna().reset_index()\n",
    "aybars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "94400                       ([40.0], [70], [10.0])\n",
       "94987       ([194.0, 96.0], [43, 2], [10.0, 10.0])\n",
       "95610     ([130.0, 200.0], [95, 59], [10.0, 10.0])\n",
       "96409        ([94.0, 12.0], [1, 63], [10.0, 10.0])\n",
       "118571                      ([38.0], [73], [10.0])\n",
       "                            ...                   \n",
       "361379    ([122.0, 110.0], [50, 51], [10.0, 10.0])\n",
       "361510                     ([202.0], [97], [10.0])\n",
       "361511                     ([202.0], [97], [10.0])\n",
       "361770                      ([190.5], [95], [0.0])\n",
       "361771                      ([200.0], [21], [0.0])\n",
       "Length: 5869, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "wy6wx9go-hXa"
   },
   "outputs": [],
   "source": [
    "# Build TensorFlow sequence datasets for training, validation, and test data\n",
    "seq, features_depth, problems_depth = prepare_seq(student_df)\n",
    "seq_train = seq[X_train_val.user_id.unique()]\n",
    "seq_val = seq[X_val.user_id.unique()]\n",
    "seq_test = seq[X_test.user_id.unique()]\n",
    "\n",
    "# Prepare the training, validation, and test data in the DKT input format\n",
    "tf_train, length = prepare_data(seq_train, params, features_depth, problems_depth)\n",
    "tf_val, val_length  = prepare_data(seq_val, params, features_depth, problems_depth)\n",
    "tf_test, test_length = prepare_data(seq_test, params, features_depth, problems_depth)\n",
    "\n",
    "# Calculate the length of each of the train-test-val sets and store as parameters\n",
    "params['train_size'] = int(length // params['batch_size'])\n",
    "params['val_size'] = int(val_length // params['batch_size'])\n",
    "params['test_size'] = int(test_length // params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "F7wEU4Ku-hXa"
   },
   "outputs": [],
   "source": [
    "# Custom metrics for training and testing\n",
    "class RMSE(tf.keras.metrics.RootMeanSquaredError):\n",
    "    # Our custom RMSE calls our get_target function first to remove predictions on padded values, \n",
    "    # then computes a standard RMSE metric.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(RMSE, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "def CustomMeanSquaredError(y_true, y_pred): \n",
    "    # Our custom mean squared error loss calls our get_target function first \n",
    "    # to remove predictions on padded values, then computes standard binary cross-entropy.\n",
    "    y_true, y_pred = get_target(y_true, y_pred)\n",
    "    return mse(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GqUeh5Za-hXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DKT\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, None, 209)]       0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 209)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 32)          30976     \n",
      "                                                                 \n",
      " outputs (TimeDistributed)   (None, None, 99)          3267      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,243\n",
      "Trainable params: 34,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Function for creating the model itself\n",
    "def create_model_lstm(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create an LSTM model architecture\n",
    "    inputs = tf.keras.Input(shape=(None, nb_features), name='inputs')\n",
    "\n",
    "    # We use a masking layer here to ignore our masked padding values\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "\n",
    "    # This LSTM layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    x = tf.keras.layers.LSTM(params['recurrent_units'], return_sequences=True, dropout=params['dropout_rate'])(x)\n",
    "    \n",
    "    # We use a dense layer with the sigmoid function activation to map our predictions \n",
    "    # between 0 and 1.\n",
    "    dense = tf.keras.layers.Dense(nb_skills, activation='sigmoid')\n",
    "\n",
    "    # The TimeDistributed layer takes the dense layer predictions and applies the sigmoid \n",
    "    # activation function to all time steps.\n",
    "    outputs = tf.keras.layers.TimeDistributed(dense, name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='DKT')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=CustomMeanSquaredError, \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=[RMSE()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create our DKT model using an LSTM\n",
    "dkt_lstm = create_model_lstm(int(features_depth), problems_depth, params)\n",
    "dkt_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "b4IwJu8D-hXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 11:32:53.480664: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 3s 15ms/step - loss: 13.5151 - root_mean_squared_error: 9.1883 - val_loss: 8.6320 - val_root_mean_squared_error: 9.0870\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 13.0298 - root_mean_squared_error: 8.9982 - val_loss: 8.2143 - val_root_mean_squared_error: 8.8622\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 12.5489 - root_mean_squared_error: 8.8652 - val_loss: 8.0855 - val_root_mean_squared_error: 8.7971\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 1s 11ms/step - loss: 12.4692 - root_mean_squared_error: 8.8225 - val_loss: 8.0222 - val_root_mean_squared_error: 8.7651\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 1s 12ms/step - loss: 12.3849 - root_mean_squared_error: 8.7920 - val_loss: 7.9839 - val_root_mean_squared_error: 8.7457\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 12.3920 - root_mean_squared_error: 8.7752 - val_loss: 7.9568 - val_root_mean_squared_error: 8.7319\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 12.3511 - root_mean_squared_error: 8.7651 - val_loss: 7.9365 - val_root_mean_squared_error: 8.7216\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 1s 11ms/step - loss: 12.2935 - root_mean_squared_error: 8.7569 - val_loss: 7.9211 - val_root_mean_squared_error: 8.7137\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 2s 13ms/step - loss: 12.2603 - root_mean_squared_error: 8.7479 - val_loss: 7.9097 - val_root_mean_squared_error: 8.7078\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 12.1589 - root_mean_squared_error: 8.7436 - val_loss: 7.9011 - val_root_mean_squared_error: 8.7034\n"
     ]
    }
   ],
   "source": [
    "# This line tells our training procedure to only save the best version of the model at any given time.\n",
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                  save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Let's fit our LSTM model on the training data. This cell takes 8 minutes to run on Colab.\n",
    "history = dkt_lstm.fit(tf_train, epochs=params['epochs'], steps_per_epoch=params['train_size']-1, \n",
    "                       validation_data=tf_val, validation_steps=params['val_size'],\n",
    "                       callbacks=[ckp_callback], verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QdjtTNdE-hXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 3ms/step - loss: 8.5144 - root_mean_squared_error: 8.7423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 8.514423370361328, 'root_mean_squared_error': 8.742298126220703}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the LSTM model with the best performance, and evaluate it on the test set. \n",
    "dkt_lstm.load_weights(params['best_model_weights'])\n",
    "dkt_lstm.evaluate(tf_test, steps=params['test_size'], verbose=params['verbose'], return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fwx3OS_r-hXa"
   },
   "source": [
    "### b) Justification of design choices (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a4E0pvtA-hXb"
   },
   "source": [
    "In your model architecture, how did you construct your model inputs and outputs? Why did you design it this way?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXfzh1U4-hXb"
   },
   "source": [
    "Instead of skills, as in the course, we use here analogously the problem IDs, encoding it in a one-hot vector. However, since we now care about the score instead of a correct/wrong label, we are now dealing with a regression problem instead of classification. Therefore, we cannot use for inputs a one-hot representation of the tuple (problem_id, label) and we rely instead on a representation similar to the target's one, i.e. concatenating the one-hot encoded vector of problem_ids and the score value.\n",
    "\n",
    "The model outputs a predicted score for each of the problems, which then is recasted in get_target so to consider only non-masked values corresponding to the relevant problem. Since we frame the problem as a regression one, the score can be any value between 0 and 10. To enforce the limits, we predict an output using sigmoid, and then rescale it multiplying it by 10. Again, being an instance of regression, we train the model using a custom implementation of the Mean Squared Error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ-WI5eh-hXb"
   },
   "source": [
    "To measure model performance we use RMSE, which stems naturally from the regression framing of the problem and is very linked to the loss we used to optimize the model (MSE). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AI46TpVm-hXb"
   },
   "source": [
    "# Question 5 (30 points)\n",
    "The CEO of LernTime decides to further improve the quality of the platform. Specifically, she would like to support struggling students early on by offering them targeted interventions and to also provide advanced tasks to excellent students. She asks you to develop a model that is able to identify the very high and very low performers **early on**, i.e. after the **first 6 weeks** of their interactions with a course. For all courses, advanced tasks should be offered to the top 20% of students, while the bottom 20% of students should benefit from interventions. The overall performance of a student at the end of the course is determined by the final exam score at the end of the course, from 0 to 100 in `exam_score`.\n",
    "\n",
    "Using the dataframe `aggregated_student_df` from the already familiar mathematics course, your data scientist colleagues have already divided the students into three groups based on the procedure described above:\n",
    "\n",
    "1. intervene: students who need help (`exam_score` <= 46.5)\n",
    "2. on-track: students who are on track (`exam_score` > 46.7 and `exam_score` <= 70.5)\n",
    "3. advanced: exceptional students (`exam_score` > 70.5)\n",
    "\n",
    "They provide you the information about the clusters in the `group` column in the `aggregated_student_df` dataframe, as well as the `exam_score` each student obtained at the end of the course.\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "a) Implement and evaluate a model (using an LSTM layer) able to predict the *group* (intervene, on-track, advanced) of a student based on his/her performance during the first six weeks (total points on problems obtained each week: `week_0`, `week_1`,  ... `week_5`).\n",
    "\n",
    "b) Visualize and discuss the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "C94rXbXD-hXb",
    "outputId": "24429f58-df64-4684-ea25-7feedaf8b322"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>week_0</th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>group</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94400</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>intervene</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94987</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95610</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>advanced</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96409</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>361379</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>361510</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>361511</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>361770</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>intervene</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>361771</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>intervene</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5869 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  week_0  week_1  week_2  week_3  week_4  week_5      group  \\\n",
       "0       94400    10.0     0.0     0.0     0.0     0.0    10.0  intervene   \n",
       "1       94987    20.0     0.0    10.0     0.0     0.0     0.0   on-track   \n",
       "2       95610    20.0    10.0     0.0     0.0     0.0     0.0   advanced   \n",
       "3       96409    10.0     0.0     0.0     0.0     0.0    20.0   on-track   \n",
       "4      118571    10.0     0.0     0.0     0.0     0.0    10.0   on-track   \n",
       "...       ...     ...     ...     ...     ...     ...     ...        ...   \n",
       "5864   361379    10.0     0.0    20.0     0.0     0.0     0.0   on-track   \n",
       "5865   361510    10.0    10.0     0.0     0.0     0.0     0.0   on-track   \n",
       "5866   361511    10.0    10.0     0.0     0.0     0.0     0.0   on-track   \n",
       "5867   361770     6.5     0.0     0.0     0.0     0.0     0.0  intervene   \n",
       "5868   361771    10.0     0.0     0.0     0.0     0.0     0.0  intervene   \n",
       "\n",
       "      exam_score  \n",
       "0           34.0  \n",
       "1           63.5  \n",
       "2           71.0  \n",
       "3           63.5  \n",
       "4           51.0  \n",
       "...          ...  \n",
       "5864        63.5  \n",
       "5865        51.0  \n",
       "5866        60.5  \n",
       "5867        38.5  \n",
       "5868        39.5  \n",
       "\n",
       "[5869 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with group labels, aggregated from the original student_df.\n",
    "# The scores for week_0 through week_5 are the aggregated (summed) points of all \n",
    "# the problems the student answered that week.\n",
    "\n",
    "aggregated_student_df = pd.read_csv('data/lerntime_classification.csv')\n",
    "aggregated_student_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e4j9JVgk-hXb",
    "tags": []
   },
   "source": [
    "### a) Implementation of performance prediction model (20 points)\n",
    "Create an a time-series model (using an LSTM layer), able to predict the `group` of a student based on the interactions of the first six weeks. Train your model for 10 epochs, and use the best model callback to find the optimal model. Evaluate your model using an appropriate performance metric (simply print it). Again do not need to tune the hyperparameters of your model for this task; instead use the following settings:\n",
    "\n",
    "```params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1\n",
    "```\n",
    "\n",
    "Luckily, one of your colleagues has already implemented a skeleton for the model and therefore, you only need to add your code to this skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build df_x (the input data) and  df_y (the labels)\n",
    "nb_weeks = 6\n",
    "nb_groups = 3\n",
    "\n",
    "# df_x should become a NumPy array of size num_of_students * num_weeks * num_of_features.\n",
    "df_x = aggregated_student_df[['week_0','week_1', 'week_2', 'week_3', 'week_4', 'week_5']].values\n",
    "df_x = df_x.reshape(len(df_x), 1, nb_weeks)\n",
    "\n",
    "# preprocess labels\n",
    "df_y = aggregated_student_df['group'].replace({'intervene': 0, 'on-track': 1, 'advanced': 2}).values.reshape(-1, 1)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(df_y)\n",
    "df_y = enc.transform(df_y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(df_x, \n",
    "                                                                df_y,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=0, \n",
    "                                                                stratify=df_y)\n",
    "\n",
    "# Split the training data further into training and validation sets.\n",
    "df_x_train_val, df_x_val, df_y_train_val, df_y_val = train_test_split(df_x_train, \n",
    "                                                                      df_y_train, \n",
    "                                                                      test_size=0.2,\n",
    "                                                                      random_state=0, \n",
    "                                                                      stratify=df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4695, 1, 6)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters are fixed!\n",
    "params = {}\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create the LSTM time-series model (use one LSTM layer)\n",
    "def create_model_lstm_time_series(nb_weeks, nb_groups, params):\n",
    "    \n",
    "    # Create an LSTM model architecture.\n",
    "    inputs = tf.keras.Input(shape=(None, nb_weeks), name='inputs')\n",
    "    \n",
    "    # We use a masking layer here to ignore our masked padding values\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "\n",
    "    # This LSTM layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    # Note that return_sequences=False because we want a many-to-one architecture.\n",
    "    x = tf.keras.layers.LSTM(params['recurrent_units'], \n",
    "                             return_sequences=False, \n",
    "                             dropout=params['dropout_rate'])(x)\n",
    "    \n",
    "        \n",
    "    dense = tf.keras.layers.Dense(nb_groups, activation='softmax')\n",
    "    outputs = dense(x)\n",
    "    print(outputs.shape)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='TimeSeries')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy, \n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=[tf.keras.metrics.AUC(), 'binary_accuracy'])    \n",
    "    return model\n",
    "\n",
    "time_series_lstm = create_model_lstm_time_series(nb_weeks, nb_groups, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 5s 31ms/step - loss: 0.6180 - auc_8: 0.6717 - binary_accuracy: 0.6932 - val_loss: 0.5679 - val_auc_8: 0.7202 - val_binary_accuracy: 0.7213\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 0.5593 - auc_8: 0.7342 - binary_accuracy: 0.7209 - val_loss: 0.5486 - val_auc_8: 0.7608 - val_binary_accuracy: 0.7245\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.5423 - auc_8: 0.7689 - binary_accuracy: 0.7229 - val_loss: 0.5321 - val_auc_8: 0.7818 - val_binary_accuracy: 0.7270\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.5262 - auc_8: 0.7848 - binary_accuracy: 0.7267 - val_loss: 0.5164 - val_auc_8: 0.7943 - val_binary_accuracy: 0.7306\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.5129 - auc_8: 0.7953 - binary_accuracy: 0.7326 - val_loss: 0.5050 - val_auc_8: 0.8009 - val_binary_accuracy: 0.7409\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.5034 - auc_8: 0.8004 - binary_accuracy: 0.7389 - val_loss: 0.4978 - val_auc_8: 0.8052 - val_binary_accuracy: 0.7433\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 2s 21ms/step - loss: 0.4988 - auc_8: 0.8020 - binary_accuracy: 0.7463 - val_loss: 0.4927 - val_auc_8: 0.8087 - val_binary_accuracy: 0.7472\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 2s 20ms/step - loss: 0.4960 - auc_8: 0.8034 - binary_accuracy: 0.7454 - val_loss: 0.4912 - val_auc_8: 0.8088 - val_binary_accuracy: 0.7469\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4915 - auc_8: 0.8073 - binary_accuracy: 0.7488 - val_loss: 0.4886 - val_auc_8: 0.8123 - val_binary_accuracy: 0.7494\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.4918 - auc_8: 0.8076 - binary_accuracy: 0.7486 - val_loss: 0.4869 - val_auc_8: 0.8113 - val_binary_accuracy: 0.7526\n"
     ]
    }
   ],
   "source": [
    "# We save only the best model during the training process.\n",
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                  save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Fit the DKT LSTM model on the given data set.\n",
    "history = time_series_lstm.fit(df_x_train_val, \n",
    "                               df_y_train_val, \n",
    "                               epochs=params['epochs'],\n",
    "                               validation_data=(df_x_val, df_y_val),\n",
    "                               callbacks=[ckp_callback], \n",
    "                               verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the best version of the the trained model and compute its prediction\n",
    "time_series_lstm.load_weights(params['best_model_weights'])\n",
    "predictions = time_series_lstm.predict(df_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7410870945853052\n"
     ]
    }
   ],
   "source": [
    "# Use an appropriate error metric to evaluate the performance of your model and print the error metric\n",
    "auc = roc_auc_score(df_y_test,predictions)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AXbTAL9u-hXc"
   },
   "source": [
    "### b) Visualization and discussion (10 points)\n",
    "How well does your model perform? Provide a visualization to support your argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_coded = predictions.argmax(axis=1)\n",
    "df_y_test_coded = df_y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TMIIB_1-hXc"
   },
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix(df_y_test_coded, predictions_coded), display_labels=['intervene', 'on-track', 'advanced'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hKHK7Y4H-hXc"
   },
   "source": [
    "The model is performing reasonably well on AUC, predicting the correct label 73.7% of the time. However, we note that our model never predicts that we should intervene, which is a big problem. The model has a large bias to predicting students are on track, missing 243 students we should have intervened on as predicted on-track students. \n",
    "\n",
    "It misses more than half of the truly advanced students, predicting that 136 students are on-track when they should be advanced, but manages to correctly predict 110 students are advanced. \n",
    "\n",
    "Overall, our model is good at predicting students are on-track, getting it wrong only 10% of the time. However, it completely fails at predicting students need help, and only manages to get 44.7% of students that are advanced correctly predicted in the advanced category."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "clean_exam_coding_questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
