{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIOEeGs_DFE0"
   },
   "source": [
    "# Welcome to the MLBD final exam (Spring 2022)\n",
    "\n",
    "The exam questions are contained in this Jupyter Notebook. The `data` folder contains the data. \n",
    "\n",
    "The logistical details, rules, and guidelines pertaining to the exam are stated below.   \n",
    "\n",
    "### Timeline and Submission\n",
    "**Exam date:** July 6, 2022   \n",
    "**Exam start:** 15h15  \n",
    "**Exam end:** 18h15\n",
    "\n",
    "### Instructions\n",
    "This exam consists of two parts, a Moodle quiz with conceptual questions and programming exercises in this notebook. **Note that the Moodle quiz with the conceptual questions will be closed by 16h15, therefore please make sure to answer the conceptual questions within the first hour of the exam. To submit this notebook for the coding questions, you should upload it to Moodle (at the latest by 18h15).**\n",
    "\n",
    "In case of issues with Moodle, send your file named as \"SCIPER_Firstname_Lastname.ipynb\" via email to paola.mejia@epfl.ch, subject \"[MLBD] Exam notebook\".\n",
    "\n",
    "### Rules\n",
    "\n",
    "1. You are allowed to use any environment. We recommend using EPFL's Noto environment, accessible through the link: [https://noto.epfl.ch/](https://noto.epfl.ch/). We prepared a Python environment with all the Python packages that you might need for the exam, in the default EPFL's Noto installation. If you want to use some additional packages, feel free to install and use them in a virtual environment. In this case, it is your own responsibility to make sure that your environment is functional and that your results can be properly interpreted for grading. \n",
    "\n",
    "\n",
    "2. Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "3. When asked for plots, please include all the needed decorations: namely title, x/y-axis labels, appropriate x/y-ticks, legend, and so on. \n",
    "\n",
    "4. We will grade your notebook as is, which means that only the results showed in your evaluated code cells will be considered. Please be sure to submit a **fully-run and evaluated notebook**. We will not run the notebook for you. Interactive plots, such as those generated using `plotly`, should be **strictly avoided**.\n",
    "\n",
    "5. You can use all the online resources (including the code from the demo notebooks from the course) you want except for communication tools (emails, web chats, forums, phone, etc.). Remember, this is not a project assignment. Therefore, no teamwork is allowed.\n",
    "\n",
    "### Setup\n",
    "We intend this notebook to be completed on EPFL's Noto environment. As in past lecture exercises, you will need to use the `Tensorflow` kernel for the dependencies to be installed appropriately. Change the kernel in the upper right corner of Noto. Select `Tensorflow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jp4ln17R8BNF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_classif\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.manifold import spectral_embedding\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MnSVIV4Xu_wt",
    "tags": []
   },
   "source": [
    "## Question 1 (15 points)\n",
    "You are the Senior Data Scientist in a learning platform called LernTime. You have realized that many users stop using the platform and want to increase user retention. For this purpose, you decide to build a model to predict whether a student will stop using the learning platform or not.\n",
    "\n",
    "Your data science team built a data frame in which each row contains the aggregated features per student (calculated over the first 5 weeks of interactions) and the feature `dropout` indicates whether the student stopped using the platform (1) or not (0) before week 10.\n",
    "\n",
    "The dataframe is in the file `lerntime.csv` and contains the following features:\n",
    "- `video_time`: total video time (in minutes) \n",
    "- `num_sessions` total number of sessions\n",
    "- `num_quizzes`: total number of quizzes attempts\n",
    "- `reading_time`: total theory reading time\n",
    "- `previous_knowledge`: standardized previous knowledge\n",
    "- `browser_speed`: standardized browser speed\n",
    "- `device`:  whether the student logged in using a smartphone (1) or a computer (-1)\n",
    "- `topics`: the topics covered by the user\n",
    "- `education`: current level of education (0: middle school, 1: high school, 2: bachelor, 3: master, 4: Ph.D.).\n",
    "- `dropout`: whether the student stopped using the platform (1) or not (0) before week 5.\n",
    "\n",
    "The newest data scientist created two models with an excellent performance. As a Senior Data Scientist, you are suspicious of the results and decide to revise the code. \n",
    "\n",
    "Your task is to:\n",
    "\n",
    "a) Identify the mistakes. In the first cell, add a comment above each line in which you identify an error and explain the error.\n",
    "\n",
    "b) In the second cell, you must correct the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TtWJ4sDNvXQP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lerntime_dropout.csv')\n",
    "\n",
    "y = df['dropout']\n",
    "X = df[['video_time', 'num_sessions', 'num_quizzes', 'reading_time',\n",
    "       'previous_knowledge', 'browser_speed']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r85IEfMeCgq9"
   },
   "source": [
    "### a) Identify the mistakes in the code (10 points)\n",
    "In the following cell, add a comment above each line in which you identify an error and explain the why it is erroneous.\n",
    "Please start each of your comments with `#ERROR:`. For example:\n",
    "\n",
    "`#ERROR: the RMSE of the model is printed instead of the AUC`\n",
    "\n",
    "`print(\"The AUC of the model is: {}\".format(rmse))          `\n",
    "\n",
    "You may assume that: \n",
    "- all the features are continous and numerical. \n",
    "- the features have already been cleaned and processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Or-X9lyFFVAm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6)\n",
      "(300, 3)\n",
      "Score model 1: 0.31\n",
      "Score model 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "## 1. Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "## 2. Feature selection (Lasso)\n",
    "print(X.shape)\n",
    "lasso = Lasso(alpha=0.1, random_state=0).fit(X, y)\n",
    "selector = SelectFromModel(lasso, prefit = True)\n",
    "X = selector.transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "## 3. Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "## Model 1\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=0)\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Score model 1: {}\".format(np.round(adjusted_mutual_info_score(preds, y_test), 2)))\n",
    "\n",
    "## Model 2\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=0)\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Score model 2: {}\".format(np.round(adjusted_mutual_info_score(preds, y_test), 2)))\n",
    "\n",
    "## Discussion\n",
    "# Our second model achieved perfect results with unseen data and outperforms the first model.\n",
    "## This is because we increased the number of estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "acDnbzGiCnBD"
   },
   "source": [
    "### b) Correct the code (5 points)\n",
    "Correct all the erroneous code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7wkyo3yACcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6)\n",
      "(300, 3)\n",
      "Score model 1: 0.31\n",
      "Score model 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "y = df['dropout']\n",
    "X = df[['video_time', 'num_sessions', 'num_quizzes', 'reading_time',\n",
    "       'previous_knowledge', 'browser_speed']]\n",
    "## 1. Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "## 2. Feature selection (Lasso)\n",
    "print(X.shape)\n",
    "lasso = Lasso(alpha=0.1, random_state=0).fit(X, y)\n",
    "selector = SelectFromModel(lasso, prefit = True)\n",
    "X = selector.transform(X)\n",
    "print(X.shape)\n",
    "\n",
    "## 3. Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "## Model 1\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=0)\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Score model 1: {}\".format(np.round(adjusted_mutual_info_score(preds, y_test), 2)))\n",
    "\n",
    "## Model 2\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, random_state=0)\n",
    "clf.fit(X,y)\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Score model 2: {}\".format(np.round(adjusted_mutual_info_score(preds, y_test), 2)))\n",
    "\n",
    "## Discussion\n",
    "# Our second model achieved perfect results with unseen data and outperforms the first model.\n",
    "## This is because we increased the number of estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5Eco8WSdxOJR"
   },
   "source": [
    "# Question 2 (5 points)\n",
    "You decide to explore the data further. You are especially interested in the two features `device` and `education` and decide to explore the relationship between them.  \n",
    "\n",
    "What is the relationship between the two features `device` and `education`? Support your answer with informative metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCP4jWrE-hXV"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR DATA EXPLORATION AND INFORMATIVE METRICS HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5KVg8APc-hXW"
   },
   "source": [
    "> YOUR DISCUSSION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dkj2DKz4ymMt"
   },
   "source": [
    "# Question 3 (40 points)\n",
    "\n",
    "After having looked in more detail into the features, you decide to explore the different type of users. You want to use your knowledge from your ML4BD course and decide to cluster using Spectral Clustering. In the course, you learnt different ways of constructing the similarity graph, yielding the adjacency matrix serving as an input to the Spectral Clustering. Based on your in-depth exploration of the data, you decide to construct the similarity graph as a  *k-nearest neighbor graph*.\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "a) Write a function to compute the k-nearest neighbor graph.\n",
    "\n",
    "b) Cluster the users using Spectral Clustering and your k-nearest neighbor graph function (use 4 neighbors). Use only the features *reading_time* and *topics*. You can assume that optimal number of clusters is 2.\n",
    "\n",
    "c) Discuss the fairness of the obtained cluster solution regarding the level of education ('education')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NeRJlAdQR8Bz"
   },
   "source": [
    "## a) Computation of the k-nearest neighbor graph (17 points)\n",
    "Unfortunately, there is no k-nearest neighbor graph implementation available in scikit-learn and you therefore have to implement the function yourself. The function `'k_nearest_neighbor_graph'` takes a similarity matrix `S` as well as the number of neighbors `k` as an input an returns the adjacency matrix `W`.\n",
    "\n",
    "Note that we will not evaluate the coding efficiency of your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZKWiCDZz5uS"
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbor_graph(S, k):\n",
    "    # S: similarity matrix\n",
    "    # k: number of neighbors\n",
    "    # YOUR CODE HERE\n",
    "    W = None # CHANGE THIS\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlRJFFE80Gag",
    "outputId": "d8540c1f-bb29-4f64-a79e-755450004620"
   },
   "outputs": [],
   "source": [
    "k = 2\n",
    "# Please run this cell for evaluation purposes\n",
    "S = [[1, 0.2, 0.7, 0.1],\n",
    "     [0.2, 1, 0.8, 0.4],\n",
    "     [0.7, 0.8, 1, 0.6],\n",
    "     [0.1, 0.4, 0.6, 1]]\n",
    "\n",
    "k_nearest_neighbor_graph(S, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEkEnF930MH7",
    "outputId": "9869bb87-a80d-4904-e13c-dbd60da234e3"
   },
   "outputs": [],
   "source": [
    "# Please run this cell for evaluation purposes\n",
    "S = [[1, 0.3, 0.01, 0.1],\n",
    "     [0.3, 1, 0.8, 0.9],\n",
    "     [0.01, 0.8, 1, 0.6],\n",
    "     [0.1, 0.9, 0.6, 1]]\n",
    "\n",
    "k_nearest_neighbor_graph(S, k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DDNtQopEmTdA"
   },
   "source": [
    "## b) Spectral Clustering (15 points)\n",
    "Perform a spectral clustering using a k-nearest neighbor graph (with 4 neighbors). Use the two features `reading_time` and `topics` only. If you did not manage to solve task 3a), use a *fully connected graph* as similarity graph to obtain the adjacency matrix `W`. You can assume that the optimal number of clusters is 2. Print the obtained cluster labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf-d-ZLyAuA_"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR PERFORMING SPECTRAL CLUSTERING HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "28wBLO3ugafD"
   },
   "source": [
    "## c) Fairness of clustering solution (8 points)\n",
    "Some students approach you and say that your clustering algorithm is not fair with respect to the education level (specified in feature `education level`). You therefore decide to investigate the fairness of the obtained clustering solution. To do so, you choose an appropriate fairness metric, implement it, and apply it to compute the fairness of your clustering solution. Your further decide to visualize the obtained results in an informative manner, as a basis for your discussion with the students."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JWlX7rIp-hXY"
   },
   "source": [
    "Choose an appropriate fairness metric, implement it, and apply it to compute the fairness of your clustering solution. Justify your choice of metric and discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oDSPcKRtpLJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR THE FAIRNESS ANALYSIS HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TKc2lczt-hXY"
   },
   "source": [
    "Justify your choice of metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIfkqCpa-hXZ"
   },
   "source": [
    "> YOUR DISCUSSION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XtKbxUQ3-hXZ"
   },
   "source": [
    "Visualize your results in an informative manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdF6fv8fuPny",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR THE FAIRNESS VISUALIZATION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3q0UhnhUuV-i"
   },
   "source": [
    "Is your clustering solution fair? If yes, why? If not, why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WgY-IlIa-hXZ"
   },
   "source": [
    "> YOUR DISCUSSION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "THlqbmP_-hXZ",
    "tags": []
   },
   "source": [
    "# Question 4 (30 points)\n",
    "To improve course quality, the CEO of LernTime decides to adapt the difficulty level of the tasks to the knowledge of the students. She asks you to develop a type of knowledge tracing model able to predict the number of points a student will get on the next problem, based on the observed performance (in terms of points) on all the past problems.\n",
    "You are provided with an example data set from a mathematics course, containing the following columns: \n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| user_id | The ID of the student who is solving the problem.  | |\n",
    "| order_id | The temporal ID (timestamp) associated with the student's answer to the problem.  | |\n",
    "| relative_week | The week # since the student's first interaction with the platform.   | |\n",
    "| problem_id | The ID associated with the problem. | |\n",
    "| score | The student's performance on the problem in terms of obtained points. The maximum number of points is 10 and the minimum number of points is 0.\n",
    "\n",
    "You decide to use a Deep Knowledge Tracing model (with an LSTM layer). Unfortunately, you cannot directly use a standard DKT architecture as:\n",
    "\n",
    "1. Your data does not have skill names or IDs available, so you will have to modify DKT to predict based on problem IDs.\n",
    "\n",
    "2. Instead of predicting a binary outcome (right/wrong), your goal is to predict the number of obtained points (score).\n",
    "\n",
    "Your tasks therefore are to:\n",
    "\n",
    "a) Implement and evaluate an adjusted version of a DKT model able to predict the number of points a student will obtain on a problem.\n",
    "\n",
    "b) Justify and discuss all your design choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5fHPSwAx-hXZ",
    "outputId": "a96a12d5-08d3-40cb-9c0d-4cb62e41efc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>relative_week</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94400</td>\n",
       "      <td>163987467</td>\n",
       "      <td>0</td>\n",
       "      <td>6473</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94400</td>\n",
       "      <td>164499411</td>\n",
       "      <td>5</td>\n",
       "      <td>11893</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94987</td>\n",
       "      <td>172053491</td>\n",
       "      <td>0</td>\n",
       "      <td>37570</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94987</td>\n",
       "      <td>172092447</td>\n",
       "      <td>0</td>\n",
       "      <td>7195</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94987</td>\n",
       "      <td>172243850</td>\n",
       "      <td>2</td>\n",
       "      <td>5945</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31653</th>\n",
       "      <td>361511</td>\n",
       "      <td>175196995</td>\n",
       "      <td>1</td>\n",
       "      <td>164496</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31654</th>\n",
       "      <td>361770</td>\n",
       "      <td>175148600</td>\n",
       "      <td>0</td>\n",
       "      <td>37570</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31655</th>\n",
       "      <td>361770</td>\n",
       "      <td>175280418</td>\n",
       "      <td>1</td>\n",
       "      <td>39162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31656</th>\n",
       "      <td>361771</td>\n",
       "      <td>175138828</td>\n",
       "      <td>0</td>\n",
       "      <td>39162</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31657</th>\n",
       "      <td>361771</td>\n",
       "      <td>175280271</td>\n",
       "      <td>1</td>\n",
       "      <td>6921</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31658 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   order_id  relative_week  problem_id  score\n",
       "0        94400  163987467              0        6473   10.0\n",
       "1        94400  164499411              5       11893   10.0\n",
       "2        94987  172053491              0       37570   10.0\n",
       "3        94987  172092447              0        7195   10.0\n",
       "4        94987  172243850              2        5945   10.0\n",
       "...        ...        ...            ...         ...    ...\n",
       "31653   361511  175196995              1      164496   10.0\n",
       "31654   361770  175148600              0       37570    6.5\n",
       "31655   361770  175280418              1       39162    0.0\n",
       "31656   361771  175138828              0       39162   10.0\n",
       "31657   361771  175280271              1        6921    0.0\n",
       "\n",
       "[31658 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df = pd.read_csv('data/lerntime_kt.csv')\n",
    "student_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo1L9bbD-hXZ"
   },
   "source": [
    "### a) Implementation of (adjusted) DKT model (20 points)\n",
    "\n",
    "Fortunately, you already have code available (below) for properly training and evaluating a standard DKT model. **Modify** this code to be able to predict the number of points a student will get on the next problem, based on the observed performance (in terms of points) on all the past problems. Note that the code in its current format **will not run properly** due to the two differences mentioned above: the data set at hand does not have skill names available and your model needs to predict the total number of obtained points for a problem instead of a binary outcome.\n",
    "\n",
    "Train your model for 10 epochs, and use the best model callback to find the optimal model. Evaluate your model using **appropriate performance metric(s)** - please print your metric(s). You do not need to tune the hyperparameters of your model for this task; instead use the following settings (already provided in the code below):\n",
    "\n",
    "```\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OuWAHJDj-hXZ"
   },
   "outputs": [],
   "source": [
    "# MODIFY THE CODE BELOW TO ADJUST THE DKT IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting the data into a training and test set\n",
    "def create_iterator(data):\n",
    "    '''\n",
    "    Create an iterator to split interactions in data into train and test, with the same student not appearing in two diverse folds.\n",
    "    :param data:        Dataframe with student's interactions.\n",
    "    :return:            An iterator.\n",
    "    '''    \n",
    "    # Both passing a matrix with the raw data or just an array of indexes works\n",
    "    X = np.arange(len(data.index))\n",
    "    # Groups of interactions are identified by the user id (we do not want the same user appearing in two folds)\n",
    "    groups = data['user_id'].values \n",
    "    return model_selection.GroupShuffleSplit(n_splits=1, train_size=.8, test_size=0.2, random_state=0).split(X, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "NPOGv7vU-hXa"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters are fixed!\n",
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "0fdSScjJ-hXZ"
   },
   "outputs": [],
   "source": [
    "# Functions for building the Tensorflow input sequences for the model\n",
    "def prepare_seq(df):\n",
    "    '''\n",
    "    Extract user_id sequence in preparation for DKT. The output of this function \n",
    "    feeds into the prepare_data() function. \n",
    "    '''\n",
    "    # Enumerate skill id as a categorical variable \n",
    "    # (i.e. [32, 12, 32, 45] -> [0, 1, 0, 2])\n",
    "    #df['skill'], skill_codes = pd.factorize(df['skill_name'], sort=True)\n",
    "    #df['skill'] = df['problem_id'].map(problem_id_map)\n",
    "    df['skill'], _ = pd.factorize(df['problem_id'], sort=True)\n",
    "\n",
    "    # Cross skill id with answer to form a synthetic feature\n",
    "    #df['skill_with_answer'] = (df['skill'], df['score'])\n",
    "\n",
    "    # Convert to a sequence per user_id and shift features 1 timestep\n",
    "    seq = df.groupby('user_id').apply(lambda r: (r['skill'].values[:-1], r['score'].values[:-1], r['skill'].values[1:], r['score'].values[1:]))\n",
    "    \n",
    "    # Get max skill depth and max feature depth\n",
    "    skill_depth = df['skill'].nunique()\n",
    "    features_depth = skill_depth\n",
    "\n",
    "    return seq, int(features_depth), int(skill_depth)\n",
    "\n",
    "def dataset_map(feat_skill, feat_score, skill, label, features_depth):\n",
    "    print(feat_skill, feat_score, skill, label, features_depth)\n",
    "    # one hot encode the features but keep the score while encoding\n",
    "    # so if the score is 10 and the feature is 3, the one-hot encoding will be [0, 0, 0, 10, 0, 0, 0, 0, 0, 0]\n",
    "    features = tf.one_hot(feat_skill, depth=features_depth)\n",
    "    features = tf.concat(values=[features, tf.expand_dims(feat_score, -1)], axis=-1)\n",
    "    labels = tf.one_hot(skill, depth=features_depth)\n",
    "    labels = tf.concat(values=[labels, tf.expand_dims(label, -1)], axis=-1)\n",
    "    return (features, labels)\n",
    "\n",
    "\n",
    "def prepare_data(seq, params, features_depth, skill_depth):\n",
    "    '''\n",
    "    Manipulate the data sequences into the right format for DKT with padding by batch\n",
    "    and encoding categorical features.\n",
    "    '''\n",
    "    \n",
    "    # Get Tensorflow Dataset\n",
    "    dataset = tf.data.Dataset.from_generator(generator=lambda: seq, output_types=(tf.int32, tf.float32, tf.int32, tf.float32))\n",
    "\n",
    "    # Encode categorical features and merge skills with labels to compute target loss\n",
    "    dataset = dataset.map(\n",
    "        lambda feat_skill, feat_score, skill, label: dataset_map(feat_skill, feat_score, skill, label, features_depth),\n",
    "    )\n",
    "\n",
    "    # Pad sequences to the appropriate length per batch\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=params['batch_size'],\n",
    "        padding_values=(params['mask_value'], params['mask_value']),\n",
    "        padded_shapes=([None, None], [None, None]),\n",
    "        drop_remainder=True\n",
    "    )\n",
    "\n",
    "    return dataset.repeat(), len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the Tensorflow output sequences for the model\n",
    "def get_target(y_true, y_pred, mask_value=params['mask_value']):\n",
    "    ''' \n",
    "    Adjust y_true and y_pred to ignore predictions made using padded values.\n",
    "    '''\n",
    "    # Get skills and labels from y_true\n",
    "    mask = 1. - tf.cast(tf.equal(y_true, mask_value), y_true.dtype)\n",
    "    y_true = y_true * mask\n",
    "\n",
    "    skills, y_true = tf.split(y_true, num_or_size_splits=[-1, 1], axis=-1)\n",
    "    # Get predictions for each skill\n",
    "    y_pred = tf.reduce_sum(y_pred * skills, axis=-1, keepdims=True)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "-sUNNRrp-hXa"
   },
   "outputs": [],
   "source": [
    "# Obtain indexes for training and test sets\n",
    "train_index, test_index = next(create_iterator(student_df))\n",
    "\n",
    "# Split the data into training and test\n",
    "X_train, X_test = student_df.iloc[train_index], student_df.iloc[test_index]\n",
    "\n",
    "# Obtain indexes for training and validation sets\n",
    "train_val_index, val_index = next(create_iterator(X_train))\n",
    "\n",
    "# Split the training data into training and validation\n",
    "X_train_val, X_val = X_train.iloc[train_val_index], X_train.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "wy6wx9go-hXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", dtype=int32) Tensor(\"args_1:0\", dtype=float32) Tensor(\"args_2:0\", dtype=int32) Tensor(\"args_3:0\", dtype=float32) 100\n",
      "Tensor(\"args_0:0\", dtype=int32) Tensor(\"args_1:0\", dtype=float32) Tensor(\"args_2:0\", dtype=int32) Tensor(\"args_3:0\", dtype=float32) 100\n",
      "Tensor(\"args_0:0\", dtype=int32) Tensor(\"args_1:0\", dtype=float32) Tensor(\"args_2:0\", dtype=int32) Tensor(\"args_3:0\", dtype=float32) 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build TensorFlow sequence datasets for training, validation, and test data\n",
    "seq, features_depth, skill_depth = prepare_seq(student_df)\n",
    "seq_train = seq[X_train_val.user_id.unique()]\n",
    "seq_val = seq[X_val.user_id.unique()]\n",
    "seq_test = seq[X_test.user_id.unique()]\n",
    "\n",
    "# Prepare the training, validation, and test data in the DKT input format\n",
    "tf_train, length = prepare_data(seq_train, params, features_depth, skill_depth)\n",
    "tf_val, val_length  = prepare_data(seq_val, params, features_depth, skill_depth)\n",
    "tf_test, test_length = prepare_data(seq_test, params, features_depth, skill_depth)\n",
    "\n",
    "# Calculate the length of each of the train-test-val sets and store as parameters\n",
    "params['train_size'] = int(length // params['batch_size'])\n",
    "params['val_size'] = int(val_length // params['batch_size'])\n",
    "params['test_size'] = int(test_length // params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "F7wEU4Ku-hXa"
   },
   "outputs": [],
   "source": [
    "# Custom metrics for training and testing\n",
    "class AUC(tf.keras.metrics.AUC):\n",
    "    # Our custom AUC calls our get_target function first to remove predictions on padded values, \n",
    "    # then computes a standard AUC metric.\n",
    "    def __init__(self):\n",
    "        # We use a super constructor here just to make our metric name pretty!\n",
    "        super(AUC, self).__init__(name='auc')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(AUC, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "def CustomBinaryCrossEntropy(y_true, y_pred): \n",
    "    # Our custom binary cross entropy loss calls our get_target function first \n",
    "    # to remove predictions on padded values, then computes standard binary cross-entropy.\n",
    "    y_true, y_pred = get_target(y_true, y_pred)\n",
    "    return tf.keras.losses.binary_crossentropy()(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics for training and testing\n",
    "class RMSE(tf.keras.metrics.RootMeanSquaredError):\n",
    "    # Our custom RMSE calls our get_target function first to remove predictions on padded values, \n",
    "    # then computes a standard RMSE metric.\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(RMSE, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "def CustomMeanSquaredError(y_true, y_pred): \n",
    "    # Our custom mean squared error loss calls our get_target function first \n",
    "    # to remove predictions on padded values, then computes standard binary cross-entropy.\n",
    "    y_true, y_pred = get_target(y_true, y_pred)\n",
    "    return mse(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "GqUeh5Za-hXa"
   },
   "outputs": [],
   "source": [
    "# Function for creating the model itself\n",
    "def create_model_lstm(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create an LSTM model architecture\n",
    "    inputs = tf.keras.Input(shape=(None, nb_features+1), name='inputs')\n",
    "\n",
    "    # We use a masking layer here to ignore our masked padding values\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "\n",
    "    # This LSTM layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    x = tf.keras.layers.LSTM(params['recurrent_units'], return_sequences=True, dropout=params['dropout_rate'])(x)\n",
    "    \n",
    "    # We use a dense layer with the sigmoid function activation to map our predictions \n",
    "    # between 0 and 1.\n",
    "    dense = tf.keras.layers.Dense(nb_skills, activation=None)\n",
    "\n",
    "    # The TimeDistributed layer takes the dense layer predictions and applies the sigmoid \n",
    "    # activation function to all time steps.\n",
    "    outputs = tf.keras.layers.TimeDistributed(dense, name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='DKT')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=CustomMeanSquaredError, \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=[RMSE()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create our DKT model using an LSTM\n",
    "dkt_lstm = create_model_lstm(features_depth, skill_depth, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "b4IwJu8D-hXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 8s 66ms/step - loss: 30.6146 - root_mean_squared_error: 9.0870 - val_loss: 20.2678 - val_root_mean_squared_error: 8.0560\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 8s 72ms/step - loss: 20.4355 - root_mean_squared_error: 7.3070 - val_loss: 12.8301 - val_root_mean_squared_error: 6.3710\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 13.3475 - root_mean_squared_error: 5.8709 - val_loss: 8.5190 - val_root_mean_squared_error: 5.1728\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 9.2535 - root_mean_squared_error: 4.8418 - val_loss: 5.9683 - val_root_mean_squared_error: 4.3169\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 6.7857 - root_mean_squared_error: 4.1167 - val_loss: 4.4658 - val_root_mean_squared_error: 3.7260\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 5.2914 - root_mean_squared_error: 3.6141 - val_loss: 3.5793 - val_root_mean_squared_error: 3.3310\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 4.3625 - root_mean_squared_error: 3.2786 - val_loss: 3.0533 - val_root_mean_squared_error: 3.0741\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 7s 62ms/step - loss: 3.7514 - root_mean_squared_error: 3.0460 - val_loss: 2.7382 - val_root_mean_squared_error: 2.9103\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 3.3981 - root_mean_squared_error: 2.8992 - val_loss: 2.5470 - val_root_mean_squared_error: 2.8070\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 3.1570 - root_mean_squared_error: 2.8026 - val_loss: 2.4296 - val_root_mean_squared_error: 2.7421\n"
     ]
    }
   ],
   "source": [
    "# This line tells our training procedure to only save the best version of the model at any given time.\n",
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint('weights/bestmodel', \n",
    "                                                  save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Let's fit our LSTM model on the training data.\n",
    "history = dkt_lstm.fit(tf_train, epochs=params['epochs'], steps_per_epoch=params['train_size']-1, \n",
    "                       validation_data=tf_val, validation_steps=params['val_size'],\n",
    "                       callbacks=[ckp_callback], verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "QdjtTNdE-hXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 37ms/step - loss: 2.2990 - root_mean_squared_error: 2.6530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 2.2990212440490723, 'root_mean_squared_error': 2.652961492538452}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the LSTM model with the best performance, and evaluate it on the test set. \n",
    "dkt_lstm.load_weights(params['best_model_weights'])\n",
    "dkt_lstm.evaluate(tf_test, steps=params['test_size'], verbose=params['verbose'], return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fwx3OS_r-hXa"
   },
   "source": [
    "### b) Justification of design choices (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a4E0pvtA-hXb"
   },
   "source": [
    "In your model architecture, how did you construct your model inputs and outputs? Why did you design it this way?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXfzh1U4-hXb"
   },
   "source": [
    "> YOUR DISCUSSION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ-WI5eh-hXb"
   },
   "source": [
    "Which metric(s) did you choose to measure your model performance? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "y5cCxXxn-hXb"
   },
   "source": [
    "> YOUR DISCUSSION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AI46TpVm-hXb"
   },
   "source": [
    "# Question 5 (30 points)\n",
    "The CEO of LernTime decides to further improve the quality of the platform. Specifically, she would like to support struggling students early on by offering them targeted interventions and to also provide advanced tasks to excellent students. She asks you to develop a model that is able to identify the very high and very low performers **early on**, i.e. after the **first 6 weeks** of their interactions with a course. For all courses, advanced tasks should be offered to the top 20% of students, while the bottom 20% of students should benefit from interventions. The overall performance of a student at the end of the course is determined by the final exam score at the end of the course, from 0 to 100 in `exam_score`.\n",
    "\n",
    "Using the dataframe `aggregated_student_df` from the already familiar mathematics course, your data scientist colleagues have already divided the students into three groups based on the procedure described above:\n",
    "\n",
    "1. intervene: students who need help (`exam_score` <= 46.5)\n",
    "2. on-track: students who are on track (`exam_score` > 46.7 and `exam_score` <= 70.5)\n",
    "3. advanced: exceptional students (`exam_score` > 70.5)\n",
    "\n",
    "They provide you the information about the clusters in the `group` column in the `aggregated_student_df` dataframe, as well as the `exam_score` each student obtained at the end of the course.\n",
    "\n",
    "Your tasks are to:\n",
    "\n",
    "a) Implement and evaluate a model (using an LSTM layer) able to predict the *group* (intervene, on-track, advanced) of a student based on his/her performance during the first six weeks (total points on problems obtained each week: `week_0`, `week_1`,  ... `week_5`).\n",
    "\n",
    "b) Visualize and discuss the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "C94rXbXD-hXb",
    "outputId": "24429f58-df64-4684-ea25-7feedaf8b322"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>week_0</th>\n",
       "      <th>week_1</th>\n",
       "      <th>week_2</th>\n",
       "      <th>week_3</th>\n",
       "      <th>week_4</th>\n",
       "      <th>week_5</th>\n",
       "      <th>group</th>\n",
       "      <th>exam_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94400</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>intervene</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94987</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95610</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>advanced</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96409</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118571</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>361379</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5865</th>\n",
       "      <td>361510</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>361511</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>on-track</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>361770</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>intervene</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>361771</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>intervene</td>\n",
       "      <td>39.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5869 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  week_0  week_1  week_2  week_3  week_4  week_5      group  \\\n",
       "0       94400    10.0     0.0     0.0     0.0     0.0    10.0  intervene   \n",
       "1       94987    20.0     0.0    10.0     0.0     0.0     0.0   on-track   \n",
       "2       95610    20.0    10.0     0.0     0.0     0.0     0.0   advanced   \n",
       "3       96409    10.0     0.0     0.0     0.0     0.0    20.0   on-track   \n",
       "4      118571    10.0     0.0     0.0     0.0     0.0    10.0   on-track   \n",
       "...       ...     ...     ...     ...     ...     ...     ...        ...   \n",
       "5864   361379    10.0     0.0    20.0     0.0     0.0     0.0   on-track   \n",
       "5865   361510    10.0    10.0     0.0     0.0     0.0     0.0   on-track   \n",
       "5866   361511    10.0    10.0     0.0     0.0     0.0     0.0   on-track   \n",
       "5867   361770     6.5     0.0     0.0     0.0     0.0     0.0  intervene   \n",
       "5868   361771    10.0     0.0     0.0     0.0     0.0     0.0  intervene   \n",
       "\n",
       "      exam_score  \n",
       "0           34.0  \n",
       "1           63.5  \n",
       "2           71.0  \n",
       "3           63.5  \n",
       "4           51.0  \n",
       "...          ...  \n",
       "5864        63.5  \n",
       "5865        51.0  \n",
       "5866        60.5  \n",
       "5867        38.5  \n",
       "5868        39.5  \n",
       "\n",
       "[5869 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe with group labels, aggregated from the original student_df.\n",
    "# The scores for week_0 through week_5 are the aggregated (summed) points of all \n",
    "# the problems the student answered that week.\n",
    "\n",
    "aggregated_student_df = pd.read_csv('data/lerntime_classification.csv')\n",
    "aggregated_student_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e4j9JVgk-hXb",
    "tags": []
   },
   "source": [
    "### a) Implementation of performance prediction model (20 points)\n",
    "Create an a time-series model (using an LSTM layer), able to predict the `group` of a student based on the interactions of the first six weeks. Train your model for 10 epochs, and use the best model callback to find the optimal model. Evaluate your model using an appropriate performance metric (simply print it). Again do not need to tune the hyperparameters of your model for this task; instead use the following settings:\n",
    "\n",
    "```\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1\n",
    "```\n",
    "\n",
    "Luckily, one of your colleagues has already implemented a skeleton for the model and therefore, you only need to add your code to this skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build df_x (the input data) and  df_y (the labels)\n",
    "aggregated_student_df_copy = aggregated_student_df.copy()\n",
    "aggregated_student_df_copy['label'],_ = pd.factorize(aggregated_student_df['group'])\n",
    "# YOUR CODE FOR CONSTRUCTING THE MODEL INPUTS AND OUTPUTS HERE\n",
    "df_x = aggregated_student_df_copy[aggregated_student_df_copy.columns.difference(['user_id','label', 'group', 'exam_score'])].values\n",
    "df_y = aggregated_student_df_copy['label'].values\n",
    "df_y = OneHotEncoder().fit_transform(df_y.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5869, 6) (5869, 3)\n",
      "(5869, 6, 1) (5869, 3)\n"
     ]
    }
   ],
   "source": [
    "print(df_x.shape, df_y.shape)\n",
    "df_x = df_x.reshape(-1,6,1)\n",
    "print(df_x.shape, df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(df_x, \n",
    "                                                                df_y,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=0, \n",
    "                                                                stratify=df_y)\n",
    "\n",
    "# Split the training data further into training and validation sets.\n",
    "df_x_train_val, df_x_val, df_y_train_val, df_y_val = train_test_split(df_x_train, \n",
    "                                                                      df_y_train, \n",
    "                                                                      test_size=0.2,\n",
    "                                                                      random_state=0, \n",
    "                                                                      stratify=df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters are fixed!\n",
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0\n",
    "params['verbose'] = 1\n",
    "params['best_model_weights'] = 'weights/bestmodel' \n",
    "params['optimizer'] = 'adam'\n",
    "params['recurrent_units'] = 32\n",
    "params['epochs'] = 10\n",
    "params['dropout_rate'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LSTM time-series model (use one LSTM layer)\n",
    "\n",
    "# YOUR CODE FOR BUILDING THE MODEL HERE\n",
    "# Function for creating the model itself\n",
    "def create_model_lstm(feature_size, week_size, label_size, params):\n",
    "    \n",
    "    # Create an LSTM model architecture\n",
    "    inputs = tf.keras.Input(shape=(week_size, feature_size), name='inputs')\n",
    "\n",
    "    # This LSTM layer is the crux of the model; we use our parameters to specify\n",
    "    # what this layer should look like (# of recurrent_units, fraction of dropout).\n",
    "    x = tf.keras.layers.LSTM(params['recurrent_units'], return_sequences=False, dropout=params['dropout_rate'])(inputs)\n",
    "    \n",
    "    # We use a dense layer with the sigmoid function activation to map our predictions \n",
    "    # between 0 and 1.\n",
    "    outputs = tf.keras.layers.Dense(label_size, activation='softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='time_series_lstm')\n",
    "\n",
    "    # Compile the model with our loss functions, optimizer, and metrics.\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy, \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=[tf.keras.metrics.AUC(), 'binary_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create our DKT model using an LSTM\n",
    "time_series_lstm = create_model_lstm(1, 6, 3, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 6s 45ms/step - loss: 0.5678 - auc_7: 0.7352 - binary_accuracy: 0.7218 - val_loss: 0.5112 - val_auc_7: 0.7933 - val_binary_accuracy: 0.7430\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 5s 42ms/step - loss: 0.5029 - auc_7: 0.7959 - binary_accuracy: 0.7416 - val_loss: 0.4927 - val_auc_7: 0.8080 - val_binary_accuracy: 0.7607\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 5s 40ms/step - loss: 0.4955 - auc_7: 0.8009 - binary_accuracy: 0.7466 - val_loss: 0.4844 - val_auc_7: 0.8163 - val_binary_accuracy: 0.7618\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 5s 43ms/step - loss: 0.4896 - auc_7: 0.8074 - binary_accuracy: 0.7465 - val_loss: 0.4807 - val_auc_7: 0.8174 - val_binary_accuracy: 0.7593\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 5s 46ms/step - loss: 0.4871 - auc_7: 0.8076 - binary_accuracy: 0.7462 - val_loss: 0.4789 - val_auc_7: 0.8194 - val_binary_accuracy: 0.7586\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 5s 44ms/step - loss: 0.4870 - auc_7: 0.8079 - binary_accuracy: 0.7489 - val_loss: 0.4774 - val_auc_7: 0.8199 - val_binary_accuracy: 0.7547\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 5s 44ms/step - loss: 0.4848 - auc_7: 0.8099 - binary_accuracy: 0.7454 - val_loss: 0.4774 - val_auc_7: 0.8220 - val_binary_accuracy: 0.7561\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 5s 44ms/step - loss: 0.4830 - auc_7: 0.8106 - binary_accuracy: 0.7484 - val_loss: 0.4776 - val_auc_7: 0.8213 - val_binary_accuracy: 0.7590\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 5s 44ms/step - loss: 0.4831 - auc_7: 0.8113 - binary_accuracy: 0.7461 - val_loss: 0.4738 - val_auc_7: 0.8232 - val_binary_accuracy: 0.7614\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 5s 40ms/step - loss: 0.4814 - auc_7: 0.8117 - binary_accuracy: 0.7466 - val_loss: 0.4759 - val_auc_7: 0.8180 - val_binary_accuracy: 0.7650\n"
     ]
    }
   ],
   "source": [
    "# We save only the best model during the training process.\n",
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], \n",
    "                                                  save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Fit the DKT LSTM on the given data set.\n",
    "history = time_series_lstm.fit(df_x_train_val, \n",
    "                               df_y_train_val, \n",
    "                               epochs=params['epochs'],\n",
    "                               validation_data=(df_x_val, df_y_val),\n",
    "                               callbacks=[ckp_callback], \n",
    "                               verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the best version of the the trained model and compute its prediction\n",
    "time_series_lstm.load_weights(params['best_model_weights'])\n",
    "predictions = time_series_lstm.predict(df_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0102849 , 0.6781077 , 0.31160742],\n",
       "       [0.00684707, 0.4585873 , 0.5345656 ],\n",
       "       [0.00745545, 0.56183314, 0.43071136],\n",
       "       ...,\n",
       "       [0.18925413, 0.78390527, 0.02684053],\n",
       "       [0.25910977, 0.7285849 , 0.01230534],\n",
       "       [0.05693343, 0.8741745 , 0.06889205]], dtype=float32)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.738932479949791\n"
     ]
    }
   ],
   "source": [
    "# Use an appropriate error metric to evaluate the performance of your model and print the error metric\n",
    "\n",
    "# YOUR CODE FOR EVALUATING MODEL PERFORMANCE HERE\n",
    "auc = roc_auc_score(df_y_test,predictions)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AXbTAL9u-hXc"
   },
   "source": [
    "### b) Visualization and discussion (10 points)\n",
    "How well does your model perform? Provide a visualization to support your argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TMIIB_1-hXc"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE FOR THE MODEL PERFORMANCE VISUALIZATION HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hKHK7Y4H-hXc"
   },
   "source": [
    "> YOUR DISCUSSION HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "clean_exam_coding_questions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
