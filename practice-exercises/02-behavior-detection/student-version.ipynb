{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Behavior Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Computer environments such as those based on educational games, interactive simulations, and educational platforms are providing more and more data, which can enable a personalized adaptation of the environment itself. For instance, this data can be used to train models able to detect the extent to which students are using the educational platform properly and react accordingly. Empowering platforms with these models can serve as a means for adaptive interventions that are of paramount importance to ensure no student is left behind.      \n",
    "\n",
    "The goal of this homework is to build a **behavior detector**, namely a classifier. Specifically, you are asked to build a detector able to classify the extent to which the students are off-task, i.e., whether students are performing interactions that are not related to the classroom's objectives. To this end, we will use a public data set which is stored in <code>ca1‐dataset.csv</code> in a CSV format. The dataset includes features at the grain size of all the actions that occurred during 20-second field observations for a student (so one student can occur in more than one record of the dataset). An example feature associated with one record of the dataset is the number of wrong actions made by the corresponding student in the last 20 seconds (more details on the features will be provided later). In addition to features, each record includes the \"OffTask\" label (Y or N), which is the target we ask you to predict, based on the values of the features. \n",
    "\n",
    "Specifically,  we will ask you to:\n",
    "1. **Part 1:** Explore the dataset and select up to **5** features from those in the CSV file that you think are the most predictive of the off-task label.\n",
    "2. **Part 2:** Design, fit, and interpret a Regression model for off-task prediction, based on the features you selected. \n",
    "3. **Part 3:** Design, fit, and interpret a Decision Tree classifier for off-task prediction, based on the features you selected, and investigate the impact of one hyper-parameter on the final results you obtain. \n",
    "3. **Part 4:** Design, fit, and interpret a Random Forest classifier for off-task prediction, based on the features you selected, investigate the impact of one hyper-parameter on the final results you obtain, and compare your findings with those you obtained with a single decision tree.\n",
    "4. **Part 5:** Conduct feature engineering to improve the features in the original data set, using the data in a second more fine-grained dataset we will provide to you. Specifically, you will be asked to create at least **5** new features that cannot be created using just the original data set, add the new features to the original data set, and see what impact they have on the Random Forest classifier. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "Our target is to identify that one specific behavior for students. A detailed description of features is given below. The features are related to the information on how the students interact with the system for individual actions. \n",
    "\n",
    "The description of the features for the individual raw actions in <code>ca1‐dataset.csv</code> is provided below. \n",
    "\n",
    "\n",
    "In the description, each line in the raw data set is associated with some interaction widget in the educational software which are denoted by \"Cell\". In addition, \"Production\" is the skill that the students are expected to learn. \n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "|   Avgright| Average of true feature (1 if the action is right, 0 otherwise) during observations|\n",
    "|\tAvgbug| Average of the bug feature (1 if there is a bug, 0 otherwise) during observations|\n",
    "|\tAvgpchange| Average number of changes of knowledege estimate during observations|\n",
    "|\tAvgtime| Average number of time spent during observations|\n",
    "|\tAvgtimeSDnormed| Average the (time taken – avg(cell) / SD(cell) for the last action of observations (SD is a function)|\n",
    "|\tAvgtimelast3SDnormed| Average of the (time taken – avg(cell) / SD(cell) for the last 3 action of observations (SD is a function)|\n",
    "|\tAvgtimelast5SDnormed| Average of the (time taken – avg(cell) / SD(cell) for the last 5 action of observations (SD is a function)|\n",
    "|\tAvgnotright| Average number of not right actions during observations|\n",
    "|\tAvghowmanywrong-up| Average number of total number of actions where this production was wrong|\n",
    "|\tAvgwrongpct-up| Average number of (total number of actions where this production was wrong, not just first attempt)/( number of steps where skill encountered so far (inclusive of current))|\n",
    "|\tAvgtimeperact-up| Average of timeperact-up feature (Total time so far on all actions involving this production, for all problems)|\n",
    "|\tAvgPrev3Count-up| Average count of involving in the same interface widget for the last 3 actions, during observations|\n",
    "|\tAvgPrev5Count-up| Average count of involving in the same interface widget for the last 5 actions, during observations|\n",
    "|\tAvgrecent5wrong| Average number of wrong actions in the last 5 actions|\n",
    "|\tAvgmanywrong-up| Average of the total number of wrong actions up to the current action|\n",
    "|\tUnique-id| ID of the aggregated observation (one observation aggregates interactions within a 20 second timeframe)|\n",
    "|\tnamea| ID of the student|\n",
    "|\tOffTask| Classification target |\n",
    "\n",
    "**The data set is available in the folder data**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "055d6505ba7897f19d1ebc0b68cc7b7e",
     "grade": true,
     "grade_id": "0-0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### PACKAGE IMPORTS ####\n",
    "\n",
    "# Your libraries here\n",
    "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0 Load the data set**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/ca1-dataset.csv\")\n",
    "\n",
    "# Let's see how the dataframe looks like\n",
    "print(\"length of the dataframe:\", len(df))\n",
    "print(\"first rows of the dataframe:\\n\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1 Preprocess the data\n",
    "----\n",
    "\n",
    "In this section, your goal is to understand the data and select 5 meaningful features that will be used in the later sections to predict the off-task behavior.   \n",
    "\n",
    "Specifically, you should:\n",
    "\n",
    "1. Select 5 features from the original dataset that are meaningful for the off-task prediciton task.  \n",
    "2. Justify your decision: How did you select the five features? Add visualizations (from your Exploratory Data Analysis) to support your answer. \n",
    "3. Create a function that splits the data set into X (the five features you selected) and y (target variable) and gives appropriate format to the target variable. \n",
    "4. Justify your decision: Which proportion of the data set will be used to validate the models. Why? \n",
    "5. Finally, do any other necessary preprocessing steps.\n",
    "6. Justify the changes done to the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1.1\"></a>\n",
    "### 1.1 \n",
    "\n",
    "List 5 features from the original dataset that are meaningful for the off-task prediciton task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2dee323de98624e19cc19637700a3d8",
     "grade": true,
     "grade_id": "1-1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 1.1\n",
    "\n",
    "meaningful_features = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_five = df[meaningful_features + ['OffTask']]\n",
    "df_five_id = df[['Unique-id']]\n",
    "df_five.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "Justify your decisions. How did you select the five features? Add visualizations to support your answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc4092486e467435f7ca2ac42f0ef563",
     "grade": true,
     "grade_id": "1-2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "Create a function that splits the data set into training and validation set. The target variable (y) is off-task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "129e490bd776b0bb584fc1e4a1043553",
     "grade": true,
     "grade_id": "1-3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 1.3\n",
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Splits data into X_train, X_val, y_train, y_val. \n",
    "    \n",
    "    20% of the data should be randomly assigned to the validation set. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame with the five selected features \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    X_train: DataFrame with features (training set) \n",
    "    X_val: DataFrame with features (validation set)\n",
    "    y_train: np.array with target variable (training set) (Should take values 0,1)\n",
    "    y_val: np.array with target variable (validation set) (Should take values 0,1)\n",
    "                \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(df_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## 2 Regression model\n",
    "---\n",
    "\n",
    "As seen in class, different link functions can be used in Generalized Linear Models.\n",
    "\n",
    "Select the appropriate link function to understand the relationship between OffTask (target label) and the previously selected features. \n",
    "\n",
    "In this exercise, you should:\n",
    "1. Create a regression model to explain the variable `OffTask` using the previously selected features.  \n",
    "2. Calculate accuracy of the model's predictions using the validation set. \n",
    "3. Interpret and explain the coeficients. Which features are significant? Did the model successfully describe the data? How do you know?  \n",
    "4. What changes could you do to improve the accuracy of the model (with the same type of regression model)?\n",
    "5. Implement your suggestions and re-run the model.\n",
    "6. Do you observe any change in the accuracy? Discuss your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "Create a generalized linear regression model to explain the variable `OffTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e99dd6d1e6d6701d59d36b3e816a227e",
     "grade": true,
     "grade_id": "2-1",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 2.1\n",
    "def build_regression(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Splits data into X and y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: DataFrame with features (training set) \n",
    "    y_train: np.array with target variable (training set) \n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    summary: detailed regression output\n",
    "             including coefficients and p-values associated\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clf, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg, summary = build_regression(X_train, y_train)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 \n",
    "\n",
    "Calculate the accuracy. Given the classifier `clf`, features `X_val` and labels `y_val`, calculate the prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b7c89cff15a0d6ae39f7869f0576b79",
     "grade": true,
     "grade_id": "2-2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 2.2\n",
    "def calculate_accuracy(clf, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Calculates accuracy (percentage of validation samples \n",
    "    that are correctly classified) for linear regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clr: previously trained classifier\n",
    "    X_val: DataFrame with features (validation set)\n",
    "    y_val: np.array with target variable (validation set)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    accuracy: float \n",
    "                \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"validation score of regression:\", calculate_accuracy(reg, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "Interpret and explain the coeficients. Which features are significant? Did the model successfully describe the data? How do you know? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63c78cb2b4e8cecd2ee41801648e516c",
     "grade": true,
     "grade_id": "2-3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "What changes could you do to improve the accuracy of the model (with the same type of regression model)?\n",
    "Explain and justify your decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3223e4eb50c298353999b9debfdc25a",
     "grade": true,
     "grade_id": "2-4",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 \n",
    "\n",
    "Implement your suggestions and re-run the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d54e1546637e7fb7fee485b27d22354",
     "grade": true,
     "grade_id": "2-5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "Do you observe any change in the accuracy? Discuss your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c9aaac7fa106cc693deb0f1ed56ac13",
     "grade": true,
     "grade_id": "2-6",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 3 Decision Tree\n",
    "---\n",
    "\n",
    "As mentioned during the lecture, decision trees are a classification model with a tree-like structure. In the following questions, you should:\n",
    "\n",
    "1. Train a decision tree with the original 5 features selected in question 1.1. Return the names of the three most important features.  \n",
    "2. Interpret the decision tree and feature importances based on the provided visualization.  \n",
    "3. Play with the maximum depth hyper-parameter. \n",
    "4. Interpret your results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "Train a decision tree with the original 5 features selected in question 1.1. Return the names of the three most important features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d1ca18eabddc7f943ed02dcc465f4f1",
     "grade": true,
     "grade_id": "3-1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 3.1\n",
    "def build_decision_tree(X_train, y_train, max_depth=3):\n",
    "    \"\"\"\n",
    "    Train a decision tree classifier.\n",
    "    1. Create a decision tree classifier given max_depth\n",
    "    2. Train the classifier\n",
    "    3. Get the importance of features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: DataFrame with the 5 selected features (training set) \n",
    "    y_train: np.array with target variable (training set)\n",
    "    max_depth : maximum depth of the decision tree\n",
    " \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    clf: decision tree classifier\n",
    "    feature_importance: list of the names of the 3 most important of features\n",
    "                        ordered by feature importance\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return clf, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " clf, feature_importance = build_decision_tree(X_train, y_train) # train the model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation score of decision tree:\", clf.score(X_val, y_val))\n",
    "print(\"most important features:\", feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 \n",
    "\n",
    "Interpret the decision tree and feature importances based on the provided visualization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7fcef180af1b5bf14ffa7b98473644f",
     "grade": true,
     "grade_id": "3-2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 \n",
    "\n",
    "Play with the maximum depth hyper-parameter. \n",
    "\n",
    "You might have noticed that there is an max_depth hyper-parameter in the Decision Tree classifier. \n",
    "Which max_depth parameter leads to the highest validation score?\n",
    "\n",
    "Plot the accuracy with the varying depths (choose an appropriate range). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb2c0991e1bd7d65bfe5419caf89e060",
     "grade": true,
     "grade_id": "3-3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 3.3\n",
    "def explore_max_depth(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Explore the max depth parameter\n",
    "    1. Get the accuracy score of the decision tree classifier at different depths\n",
    "    2. Plot the accuracy at different depths\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: DataFrame with features (training set) \n",
    "    X_val: DataFrame with features (validation set)\n",
    "    y_train: np.array with target variable (training set) \n",
    "    y_val: np.array with target variable (validation set)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_max_depth(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 \n",
    "\n",
    "Interpret your results. Which max_depth do you think is the best? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6815039f4b2f7dab443391364a49f47a",
     "grade": true,
     "grade_id": "3-4",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## 4 Random Forest\n",
    "---\n",
    "\n",
    "In this section, you will explore another classifier: the Random Forest classifier. As mentioned during the lecture, the Random Forest classifier combines the output of multiple decision trees in order to generate the final output.\n",
    "\n",
    "You should:\n",
    "\n",
    "1. Train a Random Forest with the original 5 features selected in question 1.1. Return the names of the three most important features.  \n",
    "2. Interpret your results.\n",
    "3. Compare the results from the three models\n",
    "4. Play with the Random Forest hyper-parameters. \n",
    "5. Interpret your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Train a Random Forest with the original 5 features selected in question 1.1. Return the names of the three most important features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f050d25a60d7b9f785a6ec954839e8a4",
     "grade": true,
     "grade_id": "4-1",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 4.1\n",
    "def build_random_forest(X_train, y_train, n_estimators=10):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier.\n",
    "    1. Create a Random Forest classifier given max_depth\n",
    "    2. Train the classifier \n",
    "    3. Extract the feature importance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: DataFrame with features (training set) \n",
    "    y_train: np.array with target variable (training set)  \n",
    "    n_estimators : the number of estimator in the random forest classifier\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    clf: random forest classifier\n",
    "    feature_importance: list of the names of the 3 most important of features\n",
    "                        ordered by feature importance\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clf, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, feature_importance = build_random_forest(X_train, y_train) \n",
    "\n",
    "print(\"validation score of random forest:\", clf.score(X_val, y_val))\n",
    "print(\"most important features:\", feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 \n",
    "\n",
    "Interpret your results. How do the top three features from the RF differ from the top features from the DT? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78d8b41bde8ae3c513cd54b9a15c18b0",
     "grade": true,
     "grade_id": "4-2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 \n",
    "\n",
    "Compare and discuss the results of three models (Regression, Decision Tree and Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47666a751937374d41261cc0986ac14e",
     "grade": true,
     "grade_id": "4-3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "\n",
    "\n",
    "As we have seen in the lecture and in the tutotial, there are multiple hyper-parameters that can be tuned in the Random Forest classifier (number of estimators, maximum depth, function to measure the quality of a split,  minimum number of samples required to be at a leaf node, minimum weighted fraction of the sum total of weights etc)\n",
    "\n",
    "Pick only **one** hyper-parameter and explore the changes in accuracy as you vary the values. \n",
    "\n",
    "Plot the accuracy with the varying values for the hyper-parameter you selected (choose an appropriate range). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7db757a384d971af0a7364f77ee6ce8b",
     "grade": true,
     "grade_id": "4-4",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 4.4\n",
    "def explore_hyperparameter(X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Explore ONE chosen hyperparameter\n",
    "    1. Get the accuracy score of Random Forest classifier \n",
    "        with varying values of ONE hyperparameter\n",
    "    2. Plot the accuracy at different values \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: DataFrame with features (training set) \n",
    "    X_val: DataFrame with features (validation set)\n",
    "    y_train: np.array with target variable (training set) \n",
    "    y_val: np.array with target variable (validation set)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_hyperparameter(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 \n",
    "\n",
    "Interpret your results. Justify your choice of hyper-parameter. What effect does the hyper-parameter selected have on the model? In this case, which is the optimal value? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29d8aa793b328daff0255503427217da",
     "grade": true,
     "grade_id": "4-5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 Creative extension\n",
    "---\n",
    "\n",
    "In this last part, we will ask you to be creative and extend the set of five features you are using. \n",
    "\n",
    "1. Conduct feature engineering to create at least 5 new features that cannot be created using just the original data set. Add the new features to the original data set. \n",
    "2. Train a Random Forest classifier with the extended set of features. \n",
    "3. Interpret your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this last part is to build a better behavior detector (classifier), using the features you selected in the part part as well as new features you will create based on the new data we are providing now to youfor this question. \n",
    "\n",
    "In the first part, you have used the dataset stored in <code>ca1‐dataset.csv</code> in a CSV format, this data set has already been aggregated by `UniqueID`. Now, you will need to play also with the dataset stored in <code>ca2‐dataset.csv</code>, in a CSV format as well.\n",
    "\n",
    "These two datasets represent the same data set, but at two different grain‐sizes. Specifically, the new data (<code>ca2‐dataset.csv</code>) represents individual raw student actions within educational software, while the previous data set (<code>ca1‐dataset.csv</code>) is at the grain size of all the actions that occurred during 20 second field observations by students. Note that the individual raw student actions are labeled with the same UniqueID labels as the observations are (each `UniqueID` corresponds to a single field observation). In this question, you must conduct feature engineering to improve the features in the original data set, using the data in the new data set. You must create at least **five** new features that cannot be created using just the original data set, and add the new features to the set of features you have selected in the first part of the homework. \n",
    "\n",
    "#### About the individual raw student actions dataset\n",
    "\n",
    "Our target is to identify that one specific behavioral for students. The detailed description of features are given as below. The features are related to the information how the student interacting with the system for individual actions. \n",
    "\n",
    "The description of the features for the individual raw actions in  <code>ca2‐dataset.csv</code> is provided below. \n",
    "\n",
    "\n",
    "| Name                   | Description                         |\n",
    "| ---------------------- | ------------------------------------------------------------ |\n",
    "| right| 1 if the action is right, 0 otherwise|\n",
    "|\tbug| 1 if there is a bug, 0 otherwise|\n",
    "|\tpknow-1| Knowledge estimation before the action|\n",
    "|\tPknow-2| Knowledge estimation after the action |\n",
    "|\tpchange| 1 if the knowledge estimation changes, 0 otherwise|\n",
    "|\ttime| Time spent in seconds|\n",
    "|\ttimeSDnormed| For last action, (time taken – avg(cell)  / SD(cell) (SD is a function)|\n",
    "|\ttimelast3SDnormed| For last 3 action, (time taken – avg(cell)  / SD(cell) (SD is a function)|\n",
    "|\ttimelast5SDnormed| For last 5 action, (time taken – avg(cell)  / SD(cell) (SD is a function)|\n",
    "|\tnotright| 1 if the action is not right, 0 otherwise|\n",
    "|\thowmanywrong-up| How many of the actions are wrong up to the current action|\n",
    "|\twrongpct-up| (total number of wrong actions)/( number of steps where skill encountered so far)|\n",
    "|\ttimeperact-up| Total time so far on all actions involving this production, for all problems|\n",
    "|\tPrev3Count-up| Count of, for each of last 3 actions, how many involved the same interface widget|\n",
    "|\tPrev5Count-up| Count of, for each of last 5 actions, how many involved the same interface widget|\n",
    "|\t recent5wrong| Of the last 5 actions, how many were wrong|\n",
    "|\tmanywrong-up | Total number of actions where this production was wrong up to the current action|\n",
    "|\tUnique-id| ID of the aggregated observation (one observation aggregates interactions within a 20 second timeframe)|\n",
    "\n",
    "\n",
    "**Please note that pknow-1 and pknow-2 only exist in second data set**\n",
    "\n",
    "**The data set is available in the folder data**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the second dataset\n",
    "df_raw = pd.read_csv(\"./data/ca2-dataset.csv\")\n",
    "\n",
    "# Let's see how the dataframe looks like\n",
    "print(\"length of the dataframe:\", len(df))\n",
    "print(\"first rows of the dataframe:\\n\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should note that the features stated in <code>ca1‐dataset.csv</code> were obtained by conducting feature engineering in <code>ca2‐dataset.csv</code>. To make it clearer, we provide you with an example. Let's consider the 55th row in the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This row has been obtained by computing the average of each column for the rows between 55 and 58 in the dataset <code>ca2‐dataset.csv</code>. Specifically, in the second dataset, **we group by the column Unique-id** to group all the rows recorded in the same 20 seconds of interactions. Then, we select the group with Unique-id equal to the one of the row above in the original dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_raw.groupby(by='Unique-id', as_index=False).mean()\n",
    "groups[groups['Unique-id'] == df.loc[52]['Unique-id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the Avgright in <code>df.iloc[52]</code> is the same we obtained by group data in the second dataset and picking the corresponding `Unique-id`. The same observation applies to the other columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 \n",
    "Create at least 5 new features from df_raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ed91aaf2da67e771553d6590350873f",
     "grade": true,
     "grade_id": "5-1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### GRADED CELL ####\n",
    "### 5.1\n",
    "def extend_features(df_five, df_raw, df_five_id = None):\n",
    "    \"\"\"\n",
    "    Create at least 5 new features from df_raw \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_five : DataFrame with processed data (5 features)\n",
    "    df_raw:   DataFrame with raw data\n",
    "    df_five_id: (optional) DataFrame with Unique-ids from df_five \n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    df_ext_five: DataFrame with extended features \n",
    "        (the five you selected in the first part + \n",
    "        the five you create here + the target off-task label)\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return df_ext_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext_five = extend_features(df_five,  df_raw, df_five_id)\n",
    "X_train, X_val, y_train, y_val = split_data(df_ext_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "\n",
    "Use the previously created `build_random_forest` function to train the models with the extended DataFrame. Print out the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28a2b4176e197f26fbd848dbbe7f2059",
     "grade": true,
     "grade_id": "5-2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3\n",
    "\n",
    "Interpret and compare your results. Did the new features improve the score? Write down your intuition about the possible reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ce8d199e5945749a0f3f1b15c52f7d3",
     "grade": true,
     "grade_id": "5-3",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_jupy",
   "language": "python",
   "name": "python_jupy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
